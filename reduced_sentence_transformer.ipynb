{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Train a Reduction Head on a SentenceTransformer Model </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"Linq-AI-Research/Linq-Embed-Mistral\"\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "MEDI_DATA_PATH = \"/home/cayjobla/ReducedEncoders/medi-data/medi-data.json\"\n",
    "EVAL_SIZE = 8000\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "OUTPUT_DIR = \"/home/cayjobla/ReducedEncoders/Linq-Embed-Mistral-reduced\"\n",
    "LOGGING_STEPS = 1\n",
    "EVAL_STEPS = 1000\n",
    "SAVE_STEPS = 1000\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Reduction Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "class Resize(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        activation_function = nn.SiLU(),\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            nn.Linear(in_features, out_features, bias=bias),\n",
    "            activation_function,\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.activation_function = activation_function.__class__.__name__\n",
    "        self.dropout = dropout\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(module, initializer_range=0.02):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        \n",
    "    def get_config_dict(self):\n",
    "        return {\n",
    "            \"in_features\": self.in_features,\n",
    "            \"out_features\": self.out_features,\n",
    "            \"bias\": self.bias,\n",
    "            \"activation_function\": self.activation_function,\n",
    "            \"dropout\": self.dropout\n",
    "        }\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Resize({self.get_config_dict()})\"\n",
    "\n",
    "class MultiResize(nn.Sequential):\n",
    "    def __init__(self, \n",
    "        sizes, \n",
    "        bias: bool = True, \n",
    "        activation_function = nn.SiLU(), \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            in_features = sizes[i]\n",
    "            out_features = sizes[i+1]\n",
    "            self.append(Resize(in_features, out_features, bias=bias, \n",
    "                                activation_function=activation_function, \n",
    "                                dropout=dropout))\n",
    "        self.sizes = sizes\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {\n",
    "            \"sizes\": self.sizes\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultiResize({self.get_config_dict()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2d68c41e764f01af24a01b289b0c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 4096, 'do_lower_case': False}) with Transformer model: MistralModel \n",
       "  (1): Pooling({'word_embedding_dimension': 4096, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if \"model\" in locals():     # Avoid CUDA out of memory error\n",
    "    del model\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(BASE_MODEL, device=DEVICE)\n",
    "model.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "# Prepare the model for inference (no training on full sentence transformer)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the reduction and reconstruction layers\n",
    "reduce = MultiResize(\n",
    "    sizes=[4096,3584,2048,1536,1024,768,512,384,256,128,64], \n",
    "    bias=True, \n",
    "    activation_function=nn.SiLU(),\n",
    "    dropout=0.1\n",
    ")\n",
    "expand = MultiResize(\n",
    "    sizes=[64,128,256,384,512,768,1024,1536,2048,3584,4096], \n",
    "    bias=True, \n",
    "    activation_function=nn.SiLU(),\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def load_medi_dataset(data_filepath):\n",
    "    with open(data_filepath, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    return Dataset.from_dict({\n",
    "        \"query\": [get_detailed_instruct(*item[\"query\"]) for item in json_data],\n",
    "        \"pos\": [item[\"pos\"][1] for item in json_data],  # No instruction for documents\n",
    "        \"neg\": [item[\"neg\"][1] for item in json_data],\n",
    "        \"task_name\": [item[\"task_name\"] for item in json_data]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'pos', 'neg', 'task_name'],\n",
       "    num_rows: 1435000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_medi_dataset(MEDI_DATA_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset[\"task_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Represent the Google question for retrieving answers;\n",
      "Query: what are the effects of blood pressure medication?\n",
      "\n",
      "['Cough.', 'Diarrhea or constipation.', 'Dizziness or lightheadedness.', 'Erection problems.', 'Feeling nervous.', 'Feeling tired, weak, drowsy, or a lack of energy.', 'Headache.', 'Nausea or vomiting.']\n",
      "\n",
      "['difficulty sleeping.', 'headaches.', 'feeling dizzy.', 'blurred vision.', 'constipation or diarrhoea.', 'feeling or being sick (nausea or vomiting)', 'dry mouth.', 'sweating.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 329817\n",
    "print(dataset[index][\"query\"], end=\"\\n\\n\")\n",
    "print(dataset[index][\"pos\"], end=\"\\n\\n\")\n",
    "print(dataset[index][\"neg\"], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'pos', 'neg', 'task_name'],\n",
       "        num_rows: 1427000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['query', 'pos', 'neg', 'task_name'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict\n",
    "\n",
    "train_indices, eval_indices = train_test_split(\n",
    "    list(range(len(dataset))),      # Indices of dataset\n",
    "    test_size=EVAL_SIZE,\n",
    "    stratify=dataset[\"task_name\"],  # Ensures proportional split of tasks\n",
    ")\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset.select(train_indices),\n",
    "    \"eval\": dataset.select(eval_indices)\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dataloader = DataLoader(dataset[\"eval\"], batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Downsample the dataset\n",
    "train_dataloader = DataLoader(dataset[\"train\"].select(list(range(0, 250))), batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dataloader = DataLoader(dataset[\"eval\"].select(list(range(0, 25))), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class UnsupervisedCosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):     # Could theoretically allow changing similarity / loss functions\n",
    "        super().__init__()\n",
    "        self.similarity_fn = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def get_similarity_scores(self, sentence_embeddings: Tensor):\n",
    "        similarity_matrix = self.similarity_fn(\n",
    "            sentence_embeddings.unsqueeze(0),\n",
    "            sentence_embeddings.unsqueeze(1)\n",
    "        )\n",
    "        indices = torch.triu_indices(*similarity_matrix.shape, offset=1)\n",
    "        return similarity_matrix[indices[0], indices[1]] \n",
    "        \n",
    "    def forward(self, sentence_embeddings: Tensor, labels: Tensor):\n",
    "        true_similarities = self.get_similarity_scores(labels)\n",
    "        predicted_similarities = self.get_similarity_scores(sentence_embeddings)\n",
    "        return self.loss_fn(predicted_similarities, true_similarities)\n",
    "\n",
    "class DimensionalReductionLoss(nn.Module):\n",
    "    def __init__(self, weights: Tensor = torch.ones(2)):\n",
    "        super().__init__()\n",
    "        self.cosine_similarity_loss = UnsupervisedCosineSimilarityLoss()\n",
    "        self.reconstruction_loss = nn.MSELoss(reduction=\"mean\")\n",
    "        self.weights = nn.Parameter(weights, requires_grad=True)\n",
    "\n",
    "    def forward(self, \n",
    "        sentence_embeddings: Tensor, \n",
    "        reconstructed_embeddings: Tensor, \n",
    "        labels: Tensor\n",
    "    ):\n",
    "        l_c = self.cosine_similarity_loss(sentence_embeddings, labels) * self.weights[0]\n",
    "        l_r = self.reconstruction_loss(reconstructed_embeddings, labels) * self.weights[1]\n",
    "        loss = l_c + l_r - .5*torch.log(self.weights[0] * self.weights[1])\n",
    "        return loss, l_c, l_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DimensionalReductionLoss(weights=torch.tensor([1., 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests on the loss functions\n",
    "full_embeddings = torch.randn(BATCH_SIZE, 4096)\n",
    "reduced_embeddings = reduce(full_embeddings)\n",
    "reconstructed_embeddings = expand(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0278, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnsupervisedCosineSimilarityLoss()(reduced_embeddings, full_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0317, grad_fn=<SubBackward0>),\n",
       " tensor(0.0278, grad_fn=<MulBackward0>),\n",
       " tensor(1.0039, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(reduced_embeddings, reconstructed_embeddings, full_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "trainable_params = list(reduce.parameters()) + list(expand.parameters()) + list(loss_fn.parameters())\n",
    "optimizer = optim.AdamW(trainable_params, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lr scheduler\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "lambda_lr = lambda step: max(0, (num_training_steps - step) / num_training_steps)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "reduce, expand, loss_fn, optimizer, lr_scheduler, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    reduce, expand, loss_fn, optimizer, lr_scheduler, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch):\n",
    "    with torch.no_grad():\n",
    "        full_embeddings = model.encode(\n",
    "            batch[\"query\"] + batch[\"pos\"] + batch[\"neg\"],\n",
    "            convert_to_tensor=True,\n",
    "            # show_progress_bar=False,\n",
    "            # device=DEVICE\n",
    "        ).detach()\n",
    "        \n",
    "        # full_embeddings = full_embeddings.to(\"cpu\")\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    # reduced_embeddings = reduce(full_embeddings.to(accelerator.device))\n",
    "    reduced_embeddings = reduce(full_embeddings)\n",
    "    reconstructed_embeddings = expand(reduced_embeddings)\n",
    "\n",
    "    # losses = loss_fn(reduced_embeddings, reconstructed_embeddings, full_embeddings.to(accelerator.device))\n",
    "    losses = loss_fn(reduced_embeddings, reconstructed_embeddings, full_embeddings)\n",
    "    # del full_embeddings, reduced_embeddings, reconstructed_embeddings\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 1, Train Loss: 4.071497440338135\n",
      "Epoch: 0, Step: 2, Train Loss: 4.1828203201293945\n",
      "Epoch: 0, Step: 3, Train Loss: 4.451157093048096\n",
      "Epoch: 0, Step: 4, Train Loss: 4.141313076019287\n",
      "Epoch: 0, Step: 5, Train Loss: 4.224001884460449\n",
      "Epoch: 0, Step: 6, Train Loss: 3.6943228244781494\n",
      "Epoch: 0, Step: 7, Train Loss: 4.0532989501953125\n",
      "Epoch: 0, Step: 8, Train Loss: 4.022622108459473\n",
      "Epoch: 0, Step: 9, Train Loss: 4.049502372741699\n",
      "Epoch: 0, Step: 10, Train Loss: 3.809208869934082\n",
      "Epoch: 0, Step: 11, Train Loss: 3.7141005992889404\n",
      "Epoch: 0, Step: 12, Train Loss: 3.728238821029663\n",
      "Epoch: 0, Step: 13, Train Loss: 4.212258338928223\n",
      "Epoch: 0, Step: 14, Train Loss: 4.07533073425293\n",
      "Epoch: 0, Step: 15, Train Loss: 4.232998371124268\n",
      "Epoch: 0, Step: 16, Train Loss: 4.016861915588379\n",
      "Epoch: 0, Step: 17, Train Loss: 4.096328258514404\n",
      "Epoch: 0, Step: 18, Train Loss: 4.021500110626221\n",
      "Epoch: 0, Step: 19, Train Loss: 4.002612113952637\n",
      "Epoch: 0, Step: 20, Train Loss: 4.289522171020508\n",
      "Epoch: 0, Step: 21, Train Loss: 3.7931835651397705\n",
      "Epoch: 0, Step: 22, Train Loss: 4.1765666007995605\n",
      "Epoch: 0, Step: 23, Train Loss: 3.7585458755493164\n",
      "Epoch: 0, Step: 24, Train Loss: 4.060914039611816\n",
      "Epoch: 0, Step: 25, Train Loss: 4.045214653015137\n",
      "Epoch: 0, Step: 26, Train Loss: 4.065745830535889\n",
      "Epoch: 0, Step: 27, Train Loss: 3.9274845123291016\n",
      "Epoch: 0, Step: 28, Train Loss: 3.8920140266418457\n",
      "Epoch: 0, Step: 29, Train Loss: 3.9586710929870605\n",
      "Epoch: 0, Step: 30, Train Loss: 4.163721561431885\n",
      "Epoch: 0, Step: 31, Train Loss: 3.863173723220825\n",
      "Epoch: 0, Step: 32, Train Loss: 4.200039863586426\n",
      "Epoch: 0, Step: 32, Eval Loss: 4.279732406139374\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "reduce.train()\n",
    "expand.train()\n",
    "loss_fn.train()\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "CLEAR_CUDA_CACHE_STEPS = 10\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in train_dataloader:\n",
    "        # Forward pass\n",
    "        loss, l_c, l_r = forward(batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()    # Update learning rate\n",
    "        global_step += 1\n",
    "\n",
    "        # Clear cuda cache\n",
    "        if global_step % CLEAR_CUDA_CACHE_STEPS == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Training logs\n",
    "        if global_step % LOGGING_STEPS == 0:\n",
    "            print(f\"Epoch: {epoch}, Step: {global_step}, Train Loss: {loss.item()}\")\n",
    "            accelerator.log({\n",
    "                \"train/epoch\": epoch,\n",
    "                \"train/global_step\": global_step,\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/cosine_sim_loss\": l_c.item(),\n",
    "                \"train/reconstruction_loss\": l_r.item(),\n",
    "                \"train/cosine_sim_weight\": loss_fn.weights[0].item(),\n",
    "                \"train/reconstruction_weight\": loss_fn.weights[1].item(),\n",
    "                \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            })\n",
    "\n",
    "        # Evaluation\n",
    "        if global_step % EVAL_STEPS == 0:\n",
    "            reduce.eval()\n",
    "            expand.eval()\n",
    "            loss_fn.eval()\n",
    "\n",
    "            eval_loss = 0\n",
    "            eval_l_c = 0\n",
    "            eval_l_r = 0\n",
    "            for eval_batch in eval_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    loss, l_c, l_r = forward(eval_batch)\n",
    "                    eval_loss += loss.item()\n",
    "                    eval_l_c += l_c.item()\n",
    "                    eval_l_r += l_r.item()\n",
    "\n",
    "            eval_loss /= len(eval_dataloader)\n",
    "            eval_l_c /= len(eval_dataloader)\n",
    "            eval_l_r /= len(eval_dataloader)\n",
    "\n",
    "            print(f\"Epoch: {epoch}, Step: {global_step}, Eval Loss: {eval_loss}\")\n",
    "            accelerator.log({\n",
    "                \"eval/loss\": eval_loss,\n",
    "                \"eval/cosine_sim_loss\": eval_l_c,\n",
    "                \"eval/reconstruction_loss\": eval_l_r,\n",
    "            })\n",
    "\n",
    "            reduce.train()\n",
    "            expand.train()\n",
    "            loss_fn.train()\n",
    "\n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            accelerator.wait_for_everyone()\n",
    "            reduce_unwrapped = accelerator.unwrap_model(reduce)\n",
    "            accelerator.save(reduce_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"reduce.pth\"))\n",
    "            expand_unwrapped = accelerator.unwrap_model(expand)\n",
    "            accelerator.save(expand_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"expand.pth\"))\n",
    "            loss_fn_unwrapped = accelerator.unwrap_model(loss_fn)\n",
    "            accelerator.save(loss_fn_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"loss_fn.pth\"))\n",
    "\n",
    "# Final evaluation\n",
    "reduce.eval()\n",
    "expand.eval()\n",
    "loss_fn.eval()\n",
    "\n",
    "eval_loss = 0\n",
    "eval_l_c = 0\n",
    "eval_l_r = 0\n",
    "for eval_batch in eval_dataloader:\n",
    "    with torch.no_grad():\n",
    "        loss, l_c, l_r = forward(eval_batch)\n",
    "        eval_loss += loss.item()\n",
    "        eval_l_c += l_c.item()\n",
    "        eval_l_r += l_r.item()\n",
    "\n",
    "eval_loss /= len(eval_dataloader)\n",
    "eval_l_c /= len(eval_dataloader)\n",
    "eval_l_r /= len(eval_dataloader)\n",
    "\n",
    "print(f\"Epoch: {epoch}, Step: {global_step}, Eval Loss: {eval_loss}\")\n",
    "accelerator.log({\n",
    "    \"eval/loss\": eval_loss,\n",
    "    \"eval/cosine_sim_loss\": eval_l_c,\n",
    "    \"eval/reconstruction_loss\": eval_l_r,\n",
    "})\n",
    "\n",
    "# Save the model\n",
    "accelerator.wait_for_everyone()\n",
    "reduce_unwrapped = accelerator.unwrap_model(reduce)\n",
    "accelerator.save(reduce_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"reduce.pth\"))\n",
    "expand_unwrapped = accelerator.unwrap_model(expand)\n",
    "accelerator.save(expand_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"expand.pth\"))\n",
    "loss_fn_unwrapped = accelerator.unwrap_model(loss_fn)\n",
    "accelerator.save(loss_fn_unwrapped.state_dict(), os.path.join(OUTPUT_DIR, \"loss_fn.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reduced_encoders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
