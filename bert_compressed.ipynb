{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Compressed Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a reduced BERT model that is trained differently. Though the ReducedBERT model performs well on the GLUE tasks when compared to the full-size BERT model, it has some limitations. \n",
    "\n",
    "1. The embeddings from the ReducedBERT model did not cluster well. Where a set of full-sized embeddings originally clustered into around 250 different groups, the reduced embeddings were clustered into only 2-3 groups. Adding some dimensionality reduction to the already reduced embeddings resulted in more clusters, but the clustering did not appear to be meaningful.\n",
    "2. Intuitively, while training the ReducedBERT reduction head on language understanding may be helpful, the base BERT model has already been trained for language understanding. Since we freeze the BERT weights anyway, we should focus on training our model head for reduction instead of MLM and NSP, trusting that the understanding from BERT will be transferred to the reduced embeddings.\n",
    "\n",
    "Our new model proposes adding both a contrastive learning objective and a reconstruction objective to our previous ReducedBERT model in place of the MLM and NSP model heads. These new objectives are designed to preserve to structure of the BERT embedding space while retaining the information present in the full-size BERT embeddings, effectively helping the embeddings cluster better. \n",
    "\n",
    "For contrastive learning, the idea is to push items that are similar together, and push items that are different apart from each other in the reduced embedding space. While this would be a potentially difficult problem with unlabeled text, we have the advantage of having the original BERT embeddings to compare against. Thus, for our contrastive loss, we get the cosine similarity between all pairs of original BERT embeddings in a training batch, and then compare those similarities against the cosine similarities of pairs of the corresponding reduced embeddings. We then push the consine similarities of the reduced embeddings to match the consine similarities of the original embeddings using an MSE loss. This essentially teaches the reduction head to keep the structure of the original embedding space in the reduced embedding space.\n",
    "\n",
    "For the reconstruction objective, we add a set of decompression layers to the end of the model into order to extract information from the reduced embeddings and recreate the full-size embeddings. We compare these reconstructed embeddings to the true full-size BERT embeddings and apply an MSE loss to try to get the reconstruction as close to the original as possible. This also teaches the reduction heads to reduce the full-size embeddings in such a way where the reduced embeddings contain all the same information as the full-size embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the intermediate reduction layer embeddings to see how they perform on GLUE tasks compared to the fully reduced model.\n",
    "# TODO: Determine how to train such that clusters are better\n",
    "#    - Maybe try using self-supervised contrastive learning, where the full-size embeddings are used as a baseline. (https://encord.com/blog/guide-to-contrastive-learning/#:~:text=NLP%20deals%20with%20the%20processing,semantic%20information%20and%20contextual%20relationships.)\n",
    "#    - Check out the papers here (https://github.com/ryanzhumich/Contrastive-Learning-NLP-Papers?tab=readme-ov-file#4-contrastive-learning-for-nlp)\n",
    "# TODO: Check if its best to train the first reduction layer first, then the second, etc., freezing the previous layers as you go.\n",
    "\n",
    "# TODO: Fix defaults from BertReducedConfig in the BertReducedForPreTraining class (and potentially other model classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import MPNetModel\n",
    "from reduced_encoders import MPNetReducedPreTrainedModel, DimReduce\n",
    "from reduced_encoders.models.mpnet_reduced.modeling_sbert import SBertPooler\n",
    "from reduced_encoders.modeling_utils import compressed_contrastive_loss\n",
    "from reduced_encoders.modeling_reduced import DimReduceLayer\n",
    "from reduced_encoders.modeling_outputs import CompressedModelForPreTrainingOutput\n",
    "\n",
    "class Decoder(nn.Sequential):\n",
    "    \"\"\"A module used during training of a compressed model that transforms the \n",
    "    reduced model embeddings back into full-size model embeddings with the goal\n",
    "    of matching the original embeddings as closely as possible.\n",
    "\n",
    "    The structure of the model closely matches that of the DimReduce module\n",
    "    \"\"\"\n",
    "    def __init__(self, config, modules=None):\n",
    "        input_size = config.reduced_size\n",
    "        self.decoding_sizes = config.reduction_sizes[-2::-1] + [config.hidden_size]\n",
    "        \n",
    "        if modules is None:\n",
    "            modules = OrderedDict()\n",
    "            for i, decoding_size in enumerate(self.decoding_sizes):   \n",
    "                modules[str(i)] = DecoderLayer(input_size, decoding_size, config)\n",
    "                input_size = decoding_size\n",
    "        elif not isinstance(modules, OrderedDict):\n",
    "            modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\n",
    "    \n",
    "        super().__init__(modules)\n",
    "\n",
    "# For now, let the DecoderLayer be the same as the DimReduceLayer\n",
    "DecoderLayer = DimReduceLayer\n",
    "\n",
    "\n",
    "class MPNetCompressedForPretraining(MPNetReducedPreTrainedModel):\n",
    "    def __init__(self, config=None, base_model=None, reduce_module=None, alpha=1, beta=1, do_contrast=True, \n",
    "                    do_reconstruction=True, **kwargs):\n",
    "        super().__init__(config)\n",
    "\n",
    "        kwargs['add_pooling_layer'] = False     # We use our own pooling instead\n",
    "        self.mpnet = base_model or MPNetModel(self.config, **kwargs)\n",
    "        self.pooler = SBertPooler(self.config)\n",
    "        self.reduce = reduce_module or DimReduce(self.config)\n",
    "\n",
    "        self.do_contrast = do_contrast\n",
    "        self.do_reconstruction = do_reconstruction\n",
    "\n",
    "        if do_reconstruction:\n",
    "            self.decoder = Decoder(self.config)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, \n",
    "                inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mpnet(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output, attention_mask)  \n",
    "        reduced_pooled = self.reduce(pooled_output)\n",
    "\n",
    "        # TODO: There are ways to compute the loss at each layer of the reduction, is that something possible/something we want to do?\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        contrastive_loss = 0\n",
    "        if self.do_contrast:\n",
    "            contrastive_loss = compressed_contrastive_loss(pooled_output, reduced_pooled)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        reconstruction_loss = 0\n",
    "        if self.do_reconstruction:\n",
    "            decoded_reduced_pooled_output = self.decoder(reduced_pooled)\n",
    "            reconstruction_loss = F.mse_loss(pooled_output, decoded_reduced_pooled_output)  \n",
    "\n",
    "        # Compute total loss\n",
    "        total_weighted_loss = self.alpha*contrastive_loss + self.beta*reconstruction_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            return (total_weighted_loss, contrastive_loss, reconstruction_loss, )\n",
    "\n",
    "        return CompressedModelForPreTrainingOutput(\n",
    "            loss=total_weighted_loss,\n",
    "            contrastive_loss=contrastive_loss,\n",
    "            reconstruction_loss=reconstruction_loss,\n",
    "            pooled_output=pooled_output,\n",
    "            reduced_pooled_output=reduced_pooled,\n",
    "            reconstructed_pooled_output=decoded_reduced_pooled_output if self.do_reconstruction else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNetCompressedModel(MPNetReducedPreTrainedModel):\n",
    "    def __init__(self, config=None, base_model=None, reduce_module=None, **kwargs):\n",
    "        super().__init__(config)\n",
    "\n",
    "        kwargs['add_pooling_layer'] = False     # We use our own pooling instead\n",
    "        self.mpnet = base_model or MPNetModel(self.config, **kwargs)\n",
    "        self.pooler = SBertPooler(self.config)\n",
    "        self.reduce = reduce_module or DimReduce(self.config)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, \n",
    "                inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mpnet(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output, attention_mask)  \n",
    "        reduced_pooled = self.reduce(pooled_output)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (embeddings, pooled_embeddings) + outputs[2:]\n",
    "\n",
    "        return CompressedModelForPreTrainingOutput(\n",
    "            loss=loss,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "mpnet = AutoModel.from_pretrained(checkpoint, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetReducedConfig {\n",
       "  \"_name_or_path\": \"sentence-transformers/all-mpnet-base-v2\",\n",
       "  \"architectures\": [\n",
       "    \"MPNetForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"mpnet_reduced\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooling_mode\": \"mean\",\n",
       "  \"reduced_size\": 48,\n",
       "  \"reduction_sizes\": [\n",
       "    512,\n",
       "    256,\n",
       "    128,\n",
       "    64,\n",
       "    48\n",
       "  ],\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"vocab_size\": 30527\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from reduced_encoders import MPNetReducedConfig\n",
    "\n",
    "\n",
    "base_config = AutoConfig.from_pretrained(checkpoint)\n",
    "config = MPNetReducedConfig.from_config(base_config, reduction_sizes=[512,256,128,64,48])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetCompressedForPretraining(\n",
       "  (mpnet): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "  )\n",
       "  (pooler): SBertPooler()\n",
       "  (reduce): DimReduce(\n",
       "    (0): DimReduceLayer(\n",
       "      (linear): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DimReduceLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DimReduceLayer(\n",
       "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DimReduceLayer(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DimReduceLayer(\n",
       "      (linear): Linear(in_features=64, out_features=48, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model = MPNetCompressedForPretraining(config=config, base_model=mpnet)\n",
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reduced_encoders.debug_utils import compare_weights\n",
    "\n",
    "compare_weights(mpnet, compressed_model.mpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is a test sentence that is meant to determine whether I can run text through my new compressed SBERT model. Did it work?',\n",
    "        'This is also a test sentence, but it is different from the first one. I hope this works!',\n",
    "        'A feral cat walked down the street, hoping to find a place to rest for the night',\n",
    "        'The last sentence was significantly different from the others to see where the embedding lands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4352, 0.0172, 0.3200, 0.0596, 0.3839, 0.0697])\n",
      "tensor([0.9052, 0.8000, 0.7335, 0.8493, 0.7925, 0.7880])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = compressed_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompressedModelForPreTrainingOutput(loss=tensor(0.3852), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetReducedConfig {\n",
       "  \"_name_or_path\": \"sentence-transformers/all-mpnet-base-v2\",\n",
       "  \"architectures\": [\n",
       "    \"MPNetCompressedForPretraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"mpnet_reduced\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooling_mode\": \"mean\",\n",
       "  \"reduced_size\": 48,\n",
       "  \"reduction_sizes\": [\n",
       "    512,\n",
       "    256,\n",
       "    128,\n",
       "    64,\n",
       "    48\n",
       "  ],\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"vocab_size\": 30527\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model.config.architectures = [compressed_model.__class__.__name__]\n",
    "compressed_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2467f702f63648e78bb3c4ea9055356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f780d4fdd52f4cc4af2493282d884a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ef1d04ff1428183be64cdd429fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/cayjobla/all-mpnet-base-v2-compressed/commit/805dcba348f4af29642076a2c2c4266593448b4c', commit_message='Upload tokenizer', commit_description='', oid='805dcba348f4af29642076a2c2c4266593448b4c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_checkpoint = \"cayjobla/all-mpnet-base-v2-compressed\"\n",
    "compressed_model.push_to_hub(compressed_checkpoint)\n",
    "tokenizer.push_to_hub(compressed_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Implemented model on a set of test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reduced_encoders import MPNetCompressedForPretraining\n",
    "\n",
    "compressed_checkpoint = \"cayjobla/all-mpnet-base-v2-compressed\"\n",
    "compressed_ae_model = MPNetCompressedForPretraining.from_pretrained(compressed_checkpoint, revision=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetCompressedForPretraining(\n",
       "  (mpnet): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "  )\n",
       "  (pooler): SBertPooler()\n",
       "  (reduce): DimReduce(\n",
       "    (0): DimReduceLayer(\n",
       "      (linear): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DimReduceLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DimReduceLayer(\n",
       "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DimReduceLayer(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DimReduceLayer(\n",
       "      (linear): Linear(in_features=64, out_features=48, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (0): DimReduceLayer(\n",
       "      (linear): Linear(in_features=48, out_features=64, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DimReduceLayer(\n",
       "      (linear): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DimReduceLayer(\n",
       "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DimReduceLayer(\n",
       "      (linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DimReduceLayer(\n",
       "      (linear): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_ae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/home/cayjobla/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:5]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(compressed_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  9621, 11144,  ...,  2265,  2111,     2],\n",
       "        [    0, 19469,  2007,  ...,  6024,  1014,     2],\n",
       "        [    0,  2636, 28763,  ...,  2001,  2000,     2],\n",
       "        [    0,  1041,  1014,  ...,  1004,  2869,     2],\n",
       "        [    0,  6045,  1010,  ...,  6045,  1009,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dataset['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_reduced_pooled_output shape: torch.Size([5, 768])\n",
      "pooled_output shape: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = compressed_ae_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompressedModelForPreTrainingOutput(loss=tensor(0.7705, grad_fn=<AddBackward0>), contrastive_loss=tensor(0.7636, grad_fn=<MseLossBackward0>), reconstruction_loss=tensor(0.0069, grad_fn=<MseLossBackward0>), pooled_output=tensor([[-0.0648,  0.0762,  0.0211,  ..., -0.0104, -0.0436,  0.0447],\n",
       "        [ 0.0031, -0.0040, -0.0704,  ...,  0.0212, -0.0345,  0.0325],\n",
       "        [ 0.1005, -0.1929, -0.0396,  ..., -0.0476,  0.0810,  0.0176],\n",
       "        [-0.0144, -0.1616, -0.0612,  ..., -0.0994,  0.0422, -0.0320],\n",
       "        [ 0.0237,  0.0074,  0.0287,  ..., -0.0278,  0.0005,  0.0005]],\n",
       "       grad_fn=<DivBackward0>), reduced_pooled_output=tensor([[ 3.5362e-02,  4.5518e-02,  5.2559e-02,  6.2311e-02, -3.6791e-03,\n",
       "          3.8930e-02,  3.8788e-02, -1.4240e-02, -5.5849e-02,  5.9786e-02,\n",
       "          4.9462e-02,  8.7500e-03, -5.0010e-02, -2.8244e-02, -2.7903e-02,\n",
       "         -3.7485e-02,  3.3196e-03,  4.1013e-03, -1.2987e-02,  5.1692e-02,\n",
       "          3.6447e-02, -3.9238e-02, -5.7299e-02,  8.5411e-02, -4.6529e-02,\n",
       "         -2.7258e-02,  1.6330e-02, -2.1701e-02,  4.6256e-02, -6.6362e-03,\n",
       "          4.5993e-02,  2.8551e-02,  1.2286e-02,  2.0680e-02, -3.7795e-02,\n",
       "          5.2546e-02, -6.0744e-02,  1.9157e-02, -4.5875e-02,  3.9175e-02,\n",
       "         -4.2066e-02,  6.0032e-02,  4.1057e-02,  5.8338e-02,  7.2798e-02,\n",
       "          4.1806e-02, -3.6396e-04, -3.6410e-02],\n",
       "        [ 3.5259e-02,  4.5799e-02,  5.2426e-02,  6.2157e-02, -3.8937e-03,\n",
       "          3.8796e-02,  3.8926e-02, -1.4161e-02, -5.5821e-02,  5.9455e-02,\n",
       "          4.9272e-02,  8.4960e-03, -5.0104e-02, -2.8200e-02, -2.8097e-02,\n",
       "         -3.7452e-02,  3.5121e-03,  3.9353e-03, -1.2960e-02,  5.1682e-02,\n",
       "          3.6178e-02, -3.9327e-02, -5.7360e-02,  8.5628e-02, -4.6293e-02,\n",
       "         -2.7151e-02,  1.6441e-02, -2.1574e-02,  4.6293e-02, -6.5147e-03,\n",
       "          4.5856e-02,  2.8731e-02,  1.2230e-02,  2.1029e-02, -3.7958e-02,\n",
       "          5.2201e-02, -6.0898e-02,  1.9345e-02, -4.5886e-02,  3.8847e-02,\n",
       "         -4.2411e-02,  6.0262e-02,  4.1097e-02,  5.7968e-02,  7.2932e-02,\n",
       "          4.1763e-02, -2.6391e-04, -3.6514e-02],\n",
       "        [ 3.5391e-02,  4.5741e-02,  5.2636e-02,  6.2233e-02, -3.4815e-03,\n",
       "          3.8790e-02,  3.8957e-02, -1.3805e-02, -5.5853e-02,  5.9304e-02,\n",
       "          4.9250e-02,  8.6437e-03, -5.0035e-02, -2.8073e-02, -2.8115e-02,\n",
       "         -3.7328e-02,  3.6529e-03,  3.9163e-03, -1.3118e-02,  5.1848e-02,\n",
       "          3.6193e-02, -3.9052e-02, -5.7398e-02,  8.5468e-02, -4.6396e-02,\n",
       "         -2.7007e-02,  1.6229e-02, -2.1559e-02,  4.6556e-02, -6.5950e-03,\n",
       "          4.5971e-02,  2.8722e-02,  1.2556e-02,  2.0602e-02, -3.7938e-02,\n",
       "          5.2046e-02, -6.1243e-02,  1.9361e-02, -4.5911e-02,  3.8861e-02,\n",
       "         -4.2614e-02,  6.0398e-02,  4.0915e-02,  5.8162e-02,  7.2769e-02,\n",
       "          4.1825e-02, -3.4016e-05, -3.6793e-02],\n",
       "        [ 3.5400e-02,  4.5360e-02,  5.2807e-02,  6.1823e-02, -3.4563e-03,\n",
       "          3.8840e-02,  3.9019e-02, -1.4149e-02, -5.5666e-02,  5.9310e-02,\n",
       "          4.9424e-02,  8.5786e-03, -5.0044e-02, -2.7908e-02, -2.8050e-02,\n",
       "         -3.7531e-02,  3.4594e-03,  3.8480e-03, -1.3143e-02,  5.1210e-02,\n",
       "          3.6521e-02, -3.9143e-02, -5.7346e-02,  8.5340e-02, -4.6555e-02,\n",
       "         -2.6958e-02,  1.6412e-02, -2.1646e-02,  4.6283e-02, -6.5974e-03,\n",
       "          4.5848e-02,  2.8779e-02,  1.2397e-02,  2.0496e-02, -3.7838e-02,\n",
       "          5.1841e-02, -6.0808e-02,  1.9471e-02, -4.5712e-02,  3.8831e-02,\n",
       "         -4.2016e-02,  6.0060e-02,  4.1071e-02,  5.8518e-02,  7.3257e-02,\n",
       "          4.1907e-02,  1.5632e-04, -3.6574e-02],\n",
       "        [ 3.5648e-02,  4.5584e-02,  5.2597e-02,  6.1882e-02, -3.7905e-03,\n",
       "          3.8977e-02,  3.8774e-02, -1.4246e-02, -5.5759e-02,  5.9407e-02,\n",
       "          4.9632e-02,  8.7362e-03, -4.9952e-02, -2.8362e-02, -2.8072e-02,\n",
       "         -3.7492e-02,  3.0928e-03,  3.9542e-03, -1.2831e-02,  5.1513e-02,\n",
       "          3.6646e-02, -3.9105e-02, -5.7212e-02,  8.5339e-02, -4.6352e-02,\n",
       "         -2.7120e-02,  1.6435e-02, -2.1665e-02,  4.6369e-02, -6.5666e-03,\n",
       "          4.5566e-02,  2.8557e-02,  1.2289e-02,  2.0378e-02, -3.7593e-02,\n",
       "          5.2065e-02, -6.0929e-02,  1.8957e-02, -4.5679e-02,  3.8891e-02,\n",
       "         -4.2245e-02,  5.9896e-02,  4.1035e-02,  5.8274e-02,  7.3125e-02,\n",
       "          4.1772e-02,  3.2216e-05, -3.6481e-02]], grad_fn=<GeluBackward0>), reconstructed_pooled_output=tensor([[ 0.0060,  0.0060, -0.0150,  ...,  0.0235, -0.0187, -0.0121],\n",
       "        [ 0.0060,  0.0060, -0.0150,  ...,  0.0235, -0.0187, -0.0121],\n",
       "        [ 0.0060,  0.0060, -0.0150,  ...,  0.0235, -0.0187, -0.0121],\n",
       "        [ 0.0060,  0.0060, -0.0150,  ...,  0.0235, -0.0187, -0.0121],\n",
       "        [ 0.0060,  0.0060, -0.0150,  ...,  0.0235, -0.0187, -0.0121]],\n",
       "       grad_fn=<GeluBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Loss values for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reduced_encoders import MPNetCompressedForPretraining\n",
    "\n",
    "compressed_checkpoint = \"cayjobla/all-mpnet-base-v2-compressed\"\n",
    "compressed_ae_model = MPNetCompressedForPretraining.from_pretrained(compressed_checkpoint, revision=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is a test sentence that is meant to determine whether I can run text through my new compressed SBERT model. Did it work?',\n",
    "        'This is also a test sentence, but it is different from the first one. I hope this works!',\n",
    "        'A feral cat walked down the street, hoping to find a place to rest for the night',\n",
    "        'The last sentence was significantly different from the others to see where the embedding lands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(compressed_checkpoint)\n",
    "input_values = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get embeddings\n",
    "embeddings = compressed_ae_model.base_model(**input_values)\n",
    "sentence_embeddings = compressed_ae_model.pooler(embeddings.last_hidden_state, attention_mask=input_values.attention_mask)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 48])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_embeddings = compressed_ae_model.reduce(sentence_embeddings)\n",
    "reduced_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_embeddings = compressed_ae_model.decoder(reduced_embeddings)\n",
    "reconstructed_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6461, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reduced_encoders.modeling_utils import get_cos_sim\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "full_similarity = get_cos_sim(sentence_embeddings)\n",
    "reduced_similarity = get_cos_sim(reduced_embeddings)\n",
    "MSELoss(reduction='mean')(full_similarity, reduced_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4352, 0.0172, 0.3200, 0.0596, 0.3839, 0.0697],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSELoss\n\u001b[0;32m----> 3\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m MSELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)(\u001b[43msentence_embeddings\u001b[49m, reconstructed_embeddings)\n\u001b[1;32m      4\u001b[0m reconstruction_loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "reconstruction_loss = MSELoss(reduction='sum')(sentence_embeddings, reconstructed_embeddings)\n",
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reduced_encoders import MPNetCompressedForPretraining\n",
    "\n",
    "local_model = MPNetCompressedForPretraining.from_pretrained(\"all-mpnet-base-v2-compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contrastive_weight': 0.6358375549316406,\n",
       " 'reconstruction_weight': 0.6146606802940369}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_extra_logging_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
