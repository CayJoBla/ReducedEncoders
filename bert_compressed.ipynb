{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Compressed Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a reduced BERT model that is trained differently. Where the first reduced BERT model is given a reduction head that is learned from the data, the resulting model had two different problems that we hope to solve here:\n",
    "\n",
    "1. The reduced embeddings did not cluster well. Where a set of full-sized embeddings originally clustered into around 250 different clusters, the reduced embeddings were clustered into only 3 groups. Adding some dimensionality reduction to the already reduced embeddings resulted in more clusters, but the clustering did not appear to be meaningful.\n",
    "2. After the model was fully trained, only the final reduced embedding size could be used. This means only the embeddings of size 48 could be used, and not embeddings that matched intermediate reduction layer sizes (512,256,128,64). \n",
    "\n",
    "In order to solve these problems, we first add a contrastive learning objective to our model, which should help the embeddings cluster better. The idea here is to push items that are similar together, and push items that are different apart from each other in the embedding space. While this would be a potentially difficult problem with unlabeled text, we have the advantage of having the original BERT embeddings to compare against. This means that we can use the original BERT embeddings to determine how similar certain examples are, and we can push the reduced embeddings towards similar examples or away from different examples to be similar to the original embeddings.\n",
    "\n",
    "we can also add a set of decompression layers to the end of the model. This way, we can add loss terms to model to ensure that the full-size embeddings can be extracted from the reduced size embeddings. This allows us to potentially get reduced embeddings that are more meaningful and contain all of the information of the original embeddings.\n",
    "\n",
    "The second problem is a bit more difficult to solve, but I see two potential solutions:\n",
    "1) Train each compression/decompression layer separately, freezing the trained layers before adding the next compression/decompression layer of the model. This means that for any choice of intermediate layer size, the compression and decompression layers should work. However, because we have to freeze the layers each time we add another, this reduces the potential gains of the layers working together to compress the embeddings.\n",
    "2) Find a way to get the loss terms from one intermediate compression layer to the correponding decompression layer. This way, we can train the entire model at once, and the intermediate layers should be able to be used. It could potentially be done by running through the intermediate composite parts of the compression/decompression layers multiple times, once for each reduction layer. For example, run the input through compression1->decompression1 for the first loss term, then compression1->compression2->decompression2->decompression1 for the next loss term, ..., and compression1->...->compressionN->decompressionN->...->decompression1 for the last loss term. This would increase the computation time significantly, but would potentially increase the ability of the model to learn the intermediate layers together while still allowing the intermediate layers to be used independently. We could also weight the different loss terms to give more weight to the first compression/decompression set at first, then slowly reweight in favor of the lower compression/decompression sets as the model trains. Note that when not training, the model should only run through all the compression layers first, then all the decompression layers, so that the intermediate embeddings do not need to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the intermediate reduction layer embeddings to see how they perform on GLUE tasks compared to the fully reduced model.\n",
    "# TODO: Determine how to train such that clusters are better\n",
    "#    - Maybe try using self-supervised contrastive learning, where the full-size embeddings are used as a baseline. (https://encord.com/blog/guide-to-contrastive-learning/#:~:text=NLP%20deals%20with%20the%20processing,semantic%20information%20and%20contextual%20relationships.)\n",
    "#    - Check out the papers here (https://github.com/ryanzhumich/Contrastive-Learning-NLP-Papers?tab=readme-ov-file#4-contrastive-learning-for-nlp)\n",
    "# TODO: Check if its best to train the first reduction layer first, then the second, etc., freezing the previous layers as you go.\n",
    "\n",
    "# TODO: Fix defaults from BertReducedConfig in the BertReducedForPreTraining class (and potentially other model classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from reduced_encoders.modeling_reduced import DimReduceLayer\n",
    "\n",
    "class Decoder(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Module that includes a sequence of layers that are used to undo the reduction of the input\n",
    "    embeddings. This module is used in the BertCompressedForPreTraining class to train the \n",
    "    autoencoder part of that model.\n",
    "\n",
    "    Args:\n",
    "        config (PretrainedConfig): Configuration for the base model. Should include the hidden_size\n",
    "            and reduction_sizes parameters for the dimensions of each layer of this module.\n",
    "        modules (OrderedDict): An optional ordered dictionary of modules to load the reduction\n",
    "            layers from. If not specified, the reduction layers will be randomly initialized, \n",
    "            using the sizes from the reduction_sizes parameter in the configuration.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, modules=None):\n",
    "        input_size = config.reduced_size\n",
    "        self.decoder_sizes = config.reduction_sizes.reverse()\n",
    "        \n",
    "        if modules is None:\n",
    "            modules = OrderedDict()\n",
    "            for i, decoded_size in enumerate(self.decoder_sizes):   \n",
    "                modules[str(i)] = DimReduceLayer(input_size, decoded_size, config)\n",
    "                input_size = decoded_size\n",
    "        elif not isinstance(modules, OrderedDict):\n",
    "            modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\n",
    "    \n",
    "        super().__init__(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertForPreTrainingOutput\n",
    "\n",
    "from reduced_encoders import BertReducedPreTrainedModel, DimReduce, BertReducedConfig\n",
    "from reduced_encoders.models.bert_reduced.modeling_bert_reduced import BertReducedPreTrainingHeads\n",
    "\n",
    "class BertCompressedForPretraining(BertReducedPreTrainedModel):\n",
    "    \"\"\"\n",
    "    A reduced BERT model used during pretraining. The model has both an MLM and NSP head, and \n",
    "    uses a set of decompression layers during training to ensure that the reduced embeddings\n",
    "    are able to capture all the information from the original embeddings.\n",
    "    \n",
    "    Args:\n",
    "        config (BertReducedConfig): Configuration for the reduced BERT model. \n",
    "        base_model: The base BERT model to use. If not specified, a new BERT model will be\n",
    "            initialized using the config.\n",
    "        reduce_module: The dimensionality reduction module to use. If not specified, a new\n",
    "            module will be initialized using the config.\n",
    "    \"\"\"\n",
    "    def __init__(self, config=None, base_model=None, reduce_module=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = base_model if base_model is not None else BertModel(self.config)\n",
    "        self.reduce = reduce_module if reduce_module is not None else DimReduce(self.config) # TODO: Rewrite DimReduce to get intermediate layers\n",
    "        self.decoder = Decoder() # The decoder module defined above\n",
    "        self.cls = BertReducedPreTrainingHeads(self.config)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def _get_similarities(self, embeddings):\n",
    "        \"\"\"Returned the flattened upper triangular cosine similarity matrix of the given embeddings.\"\"\"\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        similarity_matrix = cos_sim(embeddings.unsqueeze(0), embeddings.unsqueeze(1))\n",
    "        indices = torch.triu_indices(*similarity_matrix.shape, offset=1)\n",
    "        return similarity_matrix[indices[0], indices[1]]\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, \n",
    "                labels=None, next_sentence_label=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        reduced_seq, reduced_pooled = self.reduce(sequence_output), self.reduce(pooled_output) # TODO: Reduce should output intermediate embeddings\n",
    "        prediction_scores, seq_relationship_score = self.cls(reduced_seq, reduced_pooled)\n",
    "\n",
    "        print(\"Pooled output shape:\", pooled_output.shape)\n",
    "        print(\"Reduced pooled output shape:\", reduced_pooled.shape)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        # Add contrastive loss (MSE between similarity scores of pairs of embeddings -- original vs reduced)\n",
    "        # TODO: Decide whether to add loss for intermediate embeddings\n",
    "        similarity = self._get_similarities(pooled_output)\n",
    "        reduced_similarity = self._get_similarities(reduced_pooled)\n",
    "        contrastive_loss = F.mse_loss(similarity, reduced_similarity)\n",
    "\n",
    "        # Add reconstruction loss (MSE of BERT embeddings and decoded reduced embeddings)\n",
    "        # TODO: Decide whether to add loss for intermediate embeddings\n",
    "        # NOTE: NEW IDEA -- Don't run each intermediate layer again, just get MSE between intermediate reduced and decoded embeddings\n",
    "        #                   This would be much faster, and it could be computed all in the same function call for each layer\n",
    "        pooled_decoded = self.decoder(reduced_pooled)[:2]\n",
    "        reconstruction_loss = F.mse_loss(pooled_output, pooled_decoded)\n",
    "\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class CompressedModelForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Ouput type of ['MPNetCompressedForPretraining']\n",
    "\n",
    "    Args:\n",
    "        loss (torch.FloatTensor): Linear combination of the contrastive learning and the reconstruction loss. The contrastive \n",
    "            learning loss is the MSE between the cosine similarity of data pairs in the original and reduced embeddings. \n",
    "            The reconstruction loss is the MSE between the original and decoded reduced embeddings.\n",
    "        hidden_states (tuple(torch.FloatTensor)): Hidden-states of the model at the output of each layer\n",
    "        attentions (tuple(torch.FloatTensor)): Attentions weights after the attention softmax, used to compute the weighted \n",
    "            average in the self-attention heads.\n",
    "    \"\"\"\n",
    "    loss: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import MPNetModel\n",
    "from reduced_encoders import MPNetReducedPreTrainedModel, DimReduce\n",
    "from reduced_encoders.models.mpnet_reduced.modeling_sbert import SBertPooler\n",
    "\n",
    "class MPNetCompressedForPretraining(MPNetReducedPreTrainedModel):\n",
    "    def __init__(self, config=None, base_model=None, reduce_module=None, **kwargs):\n",
    "        super().__init__(config)\n",
    "\n",
    "        kwargs['add_pooling_layer'] = False     # We use our own pooling instead\n",
    "        self.mpnet = base_model or MPNetModel(self.config, **kwargs)\n",
    "        self.pooler = SBertPooler(self.config)\n",
    "        self.reduce = reduce_module or DimReduce(self.config)\n",
    "\n",
    "    def _get_similarities(self, embeddings):\n",
    "        \"\"\"Returned the flattened upper triangular cosine similarity matrix of the given embeddings.\"\"\"\n",
    "        cos_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        similarity_matrix = cos_sim(embeddings.unsqueeze(0), embeddings.unsqueeze(1))\n",
    "        indices = torch.triu_indices(*similarity_matrix.shape, offset=1)\n",
    "        return similarity_matrix[indices[0], indices[1]]\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, \n",
    "                inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mpnet(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output, attention_mask)  \n",
    "        reduced_pooled = self.reduce(pooled_output)\n",
    "\n",
    "        # TODO: There are ways to compute the loss at each layer of the reduction, is that something possible/something we want to do?\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        full_similarity = self._get_similarities(pooled_output)\n",
    "        reduced_similarity = self._get_similarities(reduced_pooled)\n",
    "        contrastive_loss = F.mse_loss(full_similarity, reduced_similarity)\n",
    "        print(full_similarity)\n",
    "        print(reduced_similarity)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        # TODO: Decide whether to implement this loss\n",
    "        reconstruction_loss = 0     \n",
    "\n",
    "        # Compute total loss\n",
    "        loss = contrastive_loss + reconstruction_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            return (embeddings, pooled_embeddings) + outputs[2:]\n",
    "\n",
    "        return CompressedModelForPreTrainingOutput(\n",
    "            loss=loss,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "mpnet = AutoModel.from_pretrained(checkpoint, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetReducedConfig {\n",
       "  \"_name_or_path\": \"sentence-transformers/all-mpnet-base-v2\",\n",
       "  \"architectures\": [\n",
       "    \"MPNetForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"mpnet_reduced\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooling_mode\": \"mean\",\n",
       "  \"reduced_size\": 48,\n",
       "  \"reduction_sizes\": [\n",
       "    512,\n",
       "    256,\n",
       "    128,\n",
       "    64,\n",
       "    48\n",
       "  ],\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"vocab_size\": 30527\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from reduced_encoders import MPNetReducedConfig\n",
    "\n",
    "\n",
    "base_config = AutoConfig.from_pretrained(checkpoint)\n",
    "config = MPNetReducedConfig.from_config(base_config, reduction_sizes=[512,256,128,64,48])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetCompressedForPretraining(\n",
       "  (mpnet): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "  )\n",
       "  (pooler): SBertPooler()\n",
       "  (reduce): DimReduce(\n",
       "    (0): DimReduceLayer(\n",
       "      (linear): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DimReduceLayer(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DimReduceLayer(\n",
       "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DimReduceLayer(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): DimReduceLayer(\n",
       "      (linear): Linear(in_features=64, out_features=48, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model = MPNetCompressedForPretraining(config=config, base_model=mpnet)\n",
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reduced_encoders.debug_utils import compare_weights\n",
    "\n",
    "compare_weights(mpnet, compressed_model.mpnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is a test sentence that is meant to determine whether I can run text through my new compressed SBERT model. Did it work?',\n",
    "        'This is also a test sentence, but it is different from the first one. I hope this works!',\n",
    "        'A feral cat walked down the street, hoping to find a place to rest for the night',\n",
    "        'The last sentence was significantly different from the others to see where the embedding lands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4352, 0.0172, 0.3200, 0.0596, 0.3839, 0.0697])\n",
      "tensor([0.9052, 0.8000, 0.7335, 0.8493, 0.7925, 0.7880])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = compressed_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompressedModelForPreTrainingOutput(loss=tensor(0.3852), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetReducedConfig {\n",
       "  \"_name_or_path\": \"sentence-transformers/all-mpnet-base-v2\",\n",
       "  \"architectures\": [\n",
       "    \"MPNetCompressedForPretraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"mpnet_reduced\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooling_mode\": \"mean\",\n",
       "  \"reduced_size\": 48,\n",
       "  \"reduction_sizes\": [\n",
       "    512,\n",
       "    256,\n",
       "    128,\n",
       "    64,\n",
       "    48\n",
       "  ],\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.31.0.dev0\",\n",
       "  \"vocab_size\": 30527\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model.config.architectures = [compressed_model.__class__.__name__]\n",
    "compressed_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2467f702f63648e78bb3c4ea9055356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f780d4fdd52f4cc4af2493282d884a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ef1d04ff1428183be64cdd429fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/cayjobla/all-mpnet-base-v2-compressed/commit/805dcba348f4af29642076a2c2c4266593448b4c', commit_message='Upload tokenizer', commit_description='', oid='805dcba348f4af29642076a2c2c4266593448b4c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_checkpoint = \"all-mpnet-base-v2-compressed\"\n",
    "compressed_model.push_to_hub(compressed_checkpoint)\n",
    "tokenizer.push_to_hub(compressed_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
