{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[ 1.3843e-01,  1.4392e-02,  5.7945e-02,  1.0208e-01,  2.3674e-01,\n",
    "         -1.2456e-03,  3.8237e-03,  3.4180e-02, -2.7562e-01,  9.5965e-02,\n",
    "         -2.5279e-02, -5.6387e-03,  1.0662e-01, -5.6819e-02, -6.3847e-02,\n",
    "         -3.7448e-02, -9.0495e-02, -7.8009e-03, -1.3608e-01, -5.2706e-05,\n",
    "         -2.8984e-02, -6.6183e-02,  1.3101e-01,  3.9965e-02, -3.3966e-02,\n",
    "          6.5255e-02, -1.9023e-02,  4.1131e-02, -6.7069e-02, -3.9647e-02,\n",
    "          1.0196e-01, -1.1253e-02,  6.2065e-03, -1.0550e-01,  5.3771e-06,\n",
    "          5.0589e-02, -4.1565e-02, -9.4339e-02,  4.2654e-02, -1.2139e-01,\n",
    "         -5.4889e-02,  4.4081e-02, -7.1823e-03, -7.6645e-02, -1.6341e-01,\n",
    "          5.2844e-03, -5.4758e-02, -9.7303e-02, -3.6718e-02,  7.0388e-02,\n",
    "          4.6580e-02, -2.1895e-01,  1.8413e-02, -9.2795e-02, -9.2540e-02,\n",
    "          2.2657e-03, -9.3285e-02,  1.0752e-01, -2.1759e-01, -5.7338e-02,\n",
    "         -8.4815e-02, -5.5572e-02,  3.0192e-02, -5.6272e-02,  2.2734e-02,\n",
    "          1.0647e-02, -4.9415e-02,  1.2685e-01,  5.1611e-02, -5.8430e-02,\n",
    "          4.5913e-02,  2.4768e-02,  1.3084e-02,  8.3807e-02,  1.2391e-01,\n",
    "          1.1542e-01, -4.5099e-02, -2.5407e-02,  9.2331e-02,  1.2048e-03,\n",
    "          1.9578e-01,  7.9580e-02,  2.6233e-02,  8.6480e-04,  6.4194e-02,\n",
    "         -3.5312e-02, -4.1409e-02,  3.9197e-02,  1.4832e-02, -9.6757e-02,\n",
    "          4.4133e-02, -6.8092e-02,  4.1335e-03,  1.3452e-01,  1.6415e-01,\n",
    "         -3.4830e-02,  9.4974e-03, -1.6763e-02,  9.2414e-03, -1.0869e-01,\n",
    "         -1.3443e-01, -7.6096e-02,  9.7772e-02, -2.0716e-02, -1.2983e-01,\n",
    "          2.1887e-02,  1.4807e-02,  2.1691e-02, -4.8535e-02,  1.9325e-02,\n",
    "         -2.2137e-01,  7.8366e-04, -5.2762e-02,  5.8053e-02, -5.0643e-02,\n",
    "         -4.4149e-02, -1.2570e-01, -5.9073e-02,  2.5491e-03,  4.8319e-02,\n",
    "         -5.8422e-02,  5.7012e-03,  6.4144e-02,  4.2621e-02, -7.9513e-02,\n",
    "         -8.3217e-03, -2.8635e-02,  4.8877e-02, -1.1933e-03,  7.2600e-02,\n",
    "         -2.0788e-02,  1.0635e-01, -1.3269e-01, -8.1354e-02,  8.1335e-02,\n",
    "         -1.8989e-01, -5.0429e-03, -1.9787e-02,  1.1287e-01, -8.2576e-02,\n",
    "         -9.0443e-02, -1.0351e-01,  6.8174e-02,  1.4275e-02, -1.1722e-01,\n",
    "         -9.9928e-02,  2.2638e-01, -1.5631e-02, -1.0156e-01, -1.8277e-02,\n",
    "          7.3189e-02, -7.1247e-03,  1.5522e-02,  7.1313e-02,  1.0342e-01,\n",
    "          9.4814e-06,  4.3368e-02,  6.7315e-02,  6.6834e-02, -2.3287e-01,\n",
    "         -2.9619e-03,  1.0511e-01,  2.7963e-03, -3.7127e-02,  5.1618e-02,\n",
    "          5.9880e-03,  5.6580e-02,  3.1119e-02,  1.1842e-01,  7.3939e-02,\n",
    "          8.0229e-02, -5.1396e-02, -5.6952e-02,  1.1683e-01,  4.2425e-03,\n",
    "          1.7112e-02, -2.7637e-03, -1.0890e-02,  1.6021e-01, -1.2583e-01,\n",
    "          8.4441e-02, -4.7473e-03, -3.2626e-02, -1.8554e-01, -5.2980e-02,\n",
    "          8.8764e-02, -6.8864e-02,  9.2975e-03, -1.3101e-01,  6.7715e-03,\n",
    "          8.4802e-02,  9.8584e-03, -5.5389e-02,  1.1852e-03,  5.3216e-02,\n",
    "          3.6560e-02, -3.3525e-02, -3.1920e-03, -7.3149e-04, -9.6105e-02,\n",
    "         -1.1541e-01, -2.1092e-02, -1.5818e-01,  2.0352e-02, -2.9003e-02,\n",
    "          1.0036e-01,  1.0614e-01,  5.7846e-02,  2.1447e-02, -9.7270e-02,\n",
    "         -4.7276e-02,  4.0026e-02,  3.8747e-02,  1.4870e-02, -2.5359e-02,\n",
    "         -5.1062e-02, -3.2935e-02,  4.7346e-03, -4.5877e-02,  6.9429e-02,\n",
    "          4.9239e-02,  1.7044e-01, -1.7863e-01, -1.3458e-02, -3.6861e-02,\n",
    "         -6.1412e-02,  1.1088e-01, -3.2604e-03,  1.1063e-01,  1.1556e-01,\n",
    "          4.4418e-02, -1.4347e-01,  2.0536e-02, -9.7114e-02,  1.7824e-01,\n",
    "         -2.4449e-02,  1.4262e-01,  8.1103e-02,  5.9719e-02, -7.5857e-02,\n",
    "         -9.7011e-02,  1.0859e-01, -6.8929e-04, -9.5534e-02, -1.5047e-01,\n",
    "          1.6022e-01,  3.3767e-02, -1.0103e-01,  1.1288e-01, -4.9287e-02,\n",
    "          2.3701e-02,  2.2363e-02, -8.3917e-03,  9.5933e-02,  8.7083e-03,\n",
    "         -3.2783e-02, -1.2214e-01,  6.4868e-02, -2.0274e-01, -1.1286e-01,\n",
    "         -8.0430e-02, -5.3814e-02, -4.1172e-02, -4.5424e-02,  2.4195e-01,\n",
    "          6.6060e-02,  7.8255e-02, -1.5791e-01,  8.4430e-02, -4.5492e-02,\n",
    "         -1.5118e-01,  2.4184e-02, -1.0583e-01, -4.7456e-02, -3.2066e-02,\n",
    "         -4.5756e-02,  1.1995e-01, -2.1109e-02, -7.2462e-02,  2.0882e-02,\n",
    "          1.4174e-03,  6.3285e-02, -8.8697e-02,  8.9866e-02,  8.2868e-02,\n",
    "          1.3593e-01,  7.0092e-02,  3.7476e-02,  6.0416e-02, -5.7924e-02,\n",
    "         -1.3180e-01,  6.0792e-02, -4.0198e-02, -5.0765e-02,  8.3741e-02,\n",
    "          1.1457e-01, -1.3086e-01, -1.5549e-02,  2.7229e-02,  9.5003e-02,\n",
    "          2.2262e-01,  7.6697e-02,  1.2174e-01,  2.9427e-02, -6.1780e-02,\n",
    "         -2.3284e-02, -4.3756e-02, -1.4964e-02,  6.3702e-03, -6.5598e-02,\n",
    "         -2.6849e-02,  2.6965e-02,  7.5990e-02, -6.2520e-02,  7.9410e-02,\n",
    "         -2.6523e-02,  3.6051e-02, -1.5753e-01, -6.8907e-02, -2.8070e-02,\n",
    "          5.2946e-02,  1.0242e-02, -6.9950e-02,  4.5123e-02, -5.6444e-02,\n",
    "          3.9515e-02, -4.9464e-02,  5.9210e-03,  6.8418e-02,  4.4863e-02,\n",
    "          8.5145e-02, -1.1847e-01,  3.2657e-02,  1.1503e-01,  9.5096e-02,\n",
    "          1.8622e-01, -6.7375e-02, -1.7129e-01,  2.4663e-02,  2.3287e-02,\n",
    "         -6.5713e-02, -1.0933e-01,  3.2050e-02,  1.8413e-02, -1.0868e-01,\n",
    "          1.5858e-03, -1.3237e-02,  7.6833e-02,  2.9519e-01, -7.6754e-02,\n",
    "         -1.0153e-02,  3.2983e-02, -1.0917e-01, -8.1519e-02,  1.2053e-02,\n",
    "          8.4874e-02,  1.4010e-02, -3.8045e-02,  3.3147e-02, -7.9660e-02,\n",
    "         -9.1574e-02, -1.6837e-02,  6.5300e-02, -1.5463e-01, -2.8345e-02,\n",
    "         -9.6493e-03,  3.9112e-03,  9.2773e-02, -9.0115e-02, -6.1527e-02,\n",
    "          1.5647e-01,  2.2132e-02, -1.7236e-02,  1.8942e-02, -4.4023e-02,\n",
    "         -8.5648e-02, -5.9910e-02, -2.0610e-02,  7.9054e-02,  2.8705e-02,\n",
    "          7.8924e-02, -1.3408e-01,  8.9694e-03, -1.3905e-02,  1.5762e-01,\n",
    "         -5.0140e-03,  7.9625e-02, -4.8758e-02, -4.5016e-02,  2.2739e-02,\n",
    "         -1.8808e-02,  1.5132e-01, -1.7795e-01,  2.0922e-02, -6.4212e-02,\n",
    "          5.0832e-02,  1.9901e-02,  1.3213e-01,  3.7426e-02,  9.5119e-02,\n",
    "          6.8872e-02,  1.6680e-01, -7.2225e-02, -6.6644e-02,  8.8258e-03,\n",
    "         -5.5869e-02, -5.3547e-02, -5.4783e-02,  1.6293e-01,  1.9823e-01,\n",
    "          2.6007e-02,  1.8276e-02,  4.1339e-02, -1.1173e-01,  6.7223e-02,\n",
    "         -9.4768e-02, -7.2545e-02, -4.8898e-02,  8.2448e-02,  1.1437e-01,\n",
    "          3.3942e-02,  2.4120e-01, -8.7061e-02,  2.4377e-02,  4.8723e-02,\n",
    "         -4.9315e-02, -1.1814e-01,  3.8711e-02,  1.1499e-02,  1.1471e-01,\n",
    "         -1.1456e-01,  1.7458e-02, -2.4040e-02,  8.0559e-02,  1.3795e-01,\n",
    "         -3.4642e-02, -6.6790e-02, -6.0768e-02, -1.7066e-01, -3.0745e-02,\n",
    "          1.1279e-02,  3.5547e-02,  3.3099e-02, -6.4488e-04, -5.2463e-02,\n",
    "         -1.8767e-01,  2.2300e-02,  3.3776e-02,  8.6301e-02,  4.5322e-02,\n",
    "         -7.0989e-02,  8.0053e-02, -8.9165e-02, -6.2591e-03, -5.2264e-02,\n",
    "          4.6569e-02, -1.1729e-01,  1.1060e-02,  6.0386e-02, -1.2753e-01,\n",
    "          7.2824e-02,  2.5105e-02,  1.2054e-01,  1.2737e-01, -9.0979e-02,\n",
    "          1.1402e-01,  3.8291e-02, -2.6255e-01,  6.4277e-02, -6.6120e-02,\n",
    "         -1.1498e-01,  3.0123e-02, -5.0036e-03,  9.0720e-02, -1.1868e-01,\n",
    "          9.8538e-02, -3.9093e-02, -3.0241e-01, -1.4491e-02,  1.4235e-04,\n",
    "         -1.2438e-01,  1.1921e-01, -5.3509e-02, -5.5835e-02,  1.9969e-02,\n",
    "          6.8141e-02, -4.8850e-03,  3.3746e-02,  6.3847e-02, -2.4482e-02,\n",
    "          2.0283e-02, -4.4306e-03,  7.1042e-02, -5.7392e-02, -1.5600e-02,\n",
    "          1.1312e-01, -8.6242e-03, -5.6188e-02,  1.4732e-01,  1.9270e-01,\n",
    "         -6.9593e-03,  5.1571e-02,  1.3901e-01, -5.4643e-02, -7.5529e-02,\n",
    "         -1.0174e-01, -2.7704e-02,  6.7120e-02, -3.5643e-02,  1.1774e-02,\n",
    "          6.6404e-02, -3.5015e-02, -4.9473e-02, -4.3960e-02,  2.8481e-02,\n",
    "         -2.1269e-01,  5.6303e-02, -2.8185e-02,  4.5498e-02,  8.8562e-02,\n",
    "         -1.8828e-02,  3.0364e-03, -1.0445e-02, -2.1055e-01, -2.5681e-02,\n",
    "          4.9384e-02, -4.6242e-02,  7.4544e-04,  1.1827e-01, -3.1030e-02,\n",
    "          4.1525e-02, -5.8566e-02,  2.6768e-03, -7.6966e-02,  3.8111e-02,\n",
    "         -3.0220e-02,  2.3184e-01,  1.3778e-01, -3.4511e-02,  1.8831e-02,\n",
    "         -3.3153e-02, -5.2012e-03, -2.1462e-02,  3.4086e-02,  3.1244e-02,\n",
    "          9.9397e-02,  7.6571e-02, -5.6897e-02, -1.8559e-01,  4.0658e-02,\n",
    "         -1.1131e-01, -1.9881e-02,  1.3019e-02,  9.0915e-02,  1.0551e-01,\n",
    "         -1.5419e-32,  4.4289e-02, -1.6660e-01,  1.1265e-01,  1.1086e-01,\n",
    "         -1.4311e-02,  1.1198e-02, -1.0322e-01,  1.8364e-02, -3.2733e-02,\n",
    "         -3.2270e-02,  8.2752e-03, -2.0653e-03,  2.1874e-04,  6.6688e-02,\n",
    "          9.9612e-02,  2.7419e-02, -1.0925e-01, -8.7036e-02,  2.0399e-03,\n",
    "          2.8134e-02, -6.5512e-02,  3.6749e-02, -5.5335e-03,  4.1112e-03,\n",
    "          1.2443e-02,  8.6269e-02, -9.7593e-02,  7.4146e-02, -7.9152e-02,\n",
    "         -2.4367e-02, -5.1913e-02, -1.0982e-01, -4.7026e-02,  1.7632e-01,\n",
    "          1.1406e-02, -1.9663e-01,  5.9274e-02, -3.2468e-03,  1.8188e-02,\n",
    "          7.9948e-02,  1.2169e-01, -4.6733e-02,  7.7958e-02,  7.7325e-02,\n",
    "          4.5716e-02, -5.5100e-02, -9.8063e-02,  1.9418e-02, -6.8467e-02,\n",
    "         -1.2982e-01,  1.3832e-01,  2.1683e-02, -1.7057e-02, -8.9812e-02,\n",
    "          8.8611e-02, -2.1594e-03, -5.4770e-02, -1.4723e-01, -4.2444e-02,\n",
    "          3.5240e-02,  7.3708e-02, -7.1264e-02, -8.5809e-02,  6.2210e-02,\n",
    "         -4.0763e-03, -8.9969e-02,  2.2704e-01, -9.4268e-02,  1.1436e-01,\n",
    "         -5.3452e-02,  2.1572e-02,  2.2260e-02,  4.5886e-03,  8.6425e-02,\n",
    "          9.3499e-03,  4.7880e-02, -1.0689e-01,  1.4661e-02, -4.3936e-02,\n",
    "          1.4772e-02, -1.1625e-01, -1.2030e-02,  1.3018e-01, -1.5181e-02,\n",
    "         -4.0662e-02, -3.9971e-02, -5.7904e-02,  1.7654e-01, -4.5774e-02,\n",
    "         -4.7946e-02,  1.1959e-01,  2.5628e-02, -6.6189e-02,  8.5911e-02,\n",
    "         -6.3396e-02, -1.3258e-02, -1.6233e-01,  1.0034e-01,  1.0969e-01,\n",
    "         -7.1125e-02, -6.6409e-02,  1.0218e-01, -1.6438e-01, -1.7278e-02,\n",
    "         -1.2227e-02,  1.4849e-02,  7.3652e-02,  2.8524e-02, -3.5444e-02,\n",
    "          3.8554e-02,  4.1547e-02,  2.1731e-02,  4.5273e-02, -6.0037e-02,\n",
    "         -1.6400e-03,  2.3711e-02,  3.4196e-02,  9.0026e-02,  3.4212e-02,\n",
    "         -2.9284e-02, -3.6985e-02, -3.0914e-01,  2.1663e-01,  1.4767e-01,\n",
    "         -9.5780e-02,  2.4215e-02,  1.0077e-01,  3.5559e-01, -3.5944e-02,\n",
    "          4.2322e-02,  2.1426e-02, -1.0355e-02,  7.3772e-07,  8.7103e-02,\n",
    "         -6.7409e-03, -1.1913e-01, -1.2855e-01, -5.2969e-03,  4.2242e-03,\n",
    "          5.0357e-03,  4.1485e-02,  2.4039e-03,  1.0127e-01,  1.1156e-01,\n",
    "          1.2978e-02, -2.3430e-02, -1.0651e-01, -6.7476e-02, -4.0151e-02,\n",
    "          4.2615e-02, -9.0598e-02, -1.0587e-01, -1.2516e-02, -1.9116e-01,\n",
    "         -4.0895e-02,  6.0272e-02, -1.2241e-01,  8.6672e-03,  1.7020e-02,\n",
    "          8.6372e-02,  7.1403e-02, -9.5271e-02, -1.5028e-01,  1.1807e-01,\n",
    "         -1.1035e-01, -1.8430e-01,  5.9283e-03,  2.1165e-02,  2.2055e-03,\n",
    "         -7.9048e-02,  5.8308e-02, -3.3503e-02,  2.4994e-01,  3.7825e-02,\n",
    "         -5.0644e-02, -1.0968e-01, -3.7491e-02,  8.7927e-02,  4.2905e-02,\n",
    "          6.8359e-02,  1.5009e-02, -2.9435e-02,  1.2415e-02,  1.0382e-01,\n",
    "         -2.3641e-02, -8.6078e-02,  4.2851e-02,  3.5536e-03, -3.8858e-02,\n",
    "          2.0054e-02,  5.8564e-02,  1.1125e-01,  5.5960e-03, -4.1410e-02,\n",
    "         -1.3621e-02, -8.4206e-02,  1.1787e-02,  8.0206e-02,  8.8670e-02,\n",
    "          2.8307e-02,  6.6963e-34, -1.6847e-01,  4.7647e-02,  1.9819e-02,\n",
    "         -4.6766e-02, -5.8538e-02, -8.0642e-02, -1.6062e-01, -8.9962e-03,\n",
    "          7.0725e-02,  5.8819e-02, -4.0935e-02]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3843e-01,  1.4392e-02,  5.7945e-02,  1.0208e-01,  2.3674e-01,\n",
       "         -1.2456e-03,  3.8237e-03,  3.4180e-02, -2.7562e-01,  9.5965e-02,\n",
       "         -2.5279e-02, -5.6387e-03,  1.0662e-01, -5.6819e-02, -6.3847e-02,\n",
       "         -3.7448e-02, -9.0495e-02, -7.8009e-03, -1.3608e-01, -5.2706e-05,\n",
       "         -2.8984e-02, -6.6183e-02,  1.3101e-01,  3.9965e-02, -3.3966e-02,\n",
       "          6.5255e-02, -1.9023e-02,  4.1131e-02, -6.7069e-02, -3.9647e-02,\n",
       "          1.0196e-01, -1.1253e-02,  6.2065e-03, -1.0550e-01,  5.3771e-06,\n",
       "          5.0589e-02, -4.1565e-02, -9.4339e-02,  4.2654e-02, -1.2139e-01,\n",
       "         -5.4889e-02,  4.4081e-02, -7.1823e-03, -7.6645e-02, -1.6341e-01,\n",
       "          5.2844e-03, -5.4758e-02, -9.7303e-02, -3.6718e-02,  7.0388e-02,\n",
       "          4.6580e-02, -2.1895e-01,  1.8413e-02, -9.2795e-02, -9.2540e-02,\n",
       "          2.2657e-03, -9.3285e-02,  1.0752e-01, -2.1759e-01, -5.7338e-02,\n",
       "         -8.4815e-02, -5.5572e-02,  3.0192e-02, -5.6272e-02,  2.2734e-02,\n",
       "          1.0647e-02, -4.9415e-02,  1.2685e-01,  5.1611e-02, -5.8430e-02,\n",
       "          4.5913e-02,  2.4768e-02,  1.3084e-02,  8.3807e-02,  1.2391e-01,\n",
       "          1.1542e-01, -4.5099e-02, -2.5407e-02,  9.2331e-02,  1.2048e-03,\n",
       "          1.9578e-01,  7.9580e-02,  2.6233e-02,  8.6480e-04,  6.4194e-02,\n",
       "         -3.5312e-02, -4.1409e-02,  3.9197e-02,  1.4832e-02, -9.6757e-02,\n",
       "          4.4133e-02, -6.8092e-02,  4.1335e-03,  1.3452e-01,  1.6415e-01,\n",
       "         -3.4830e-02,  9.4974e-03, -1.6763e-02,  9.2414e-03, -1.0869e-01,\n",
       "         -1.3443e-01, -7.6096e-02,  9.7772e-02, -2.0716e-02, -1.2983e-01,\n",
       "          2.1887e-02,  1.4807e-02,  2.1691e-02, -4.8535e-02,  1.9325e-02,\n",
       "         -2.2137e-01,  7.8366e-04, -5.2762e-02,  5.8053e-02, -5.0643e-02,\n",
       "         -4.4149e-02, -1.2570e-01, -5.9073e-02,  2.5491e-03,  4.8319e-02,\n",
       "         -5.8422e-02,  5.7012e-03,  6.4144e-02,  4.2621e-02, -7.9513e-02,\n",
       "         -8.3217e-03, -2.8635e-02,  4.8877e-02, -1.1933e-03,  7.2600e-02,\n",
       "         -2.0788e-02,  1.0635e-01, -1.3269e-01, -8.1354e-02,  8.1335e-02,\n",
       "         -1.8989e-01, -5.0429e-03, -1.9787e-02,  1.1287e-01, -8.2576e-02,\n",
       "         -9.0443e-02, -1.0351e-01,  6.8174e-02,  1.4275e-02, -1.1722e-01,\n",
       "         -9.9928e-02,  2.2638e-01, -1.5631e-02, -1.0156e-01, -1.8277e-02,\n",
       "          7.3189e-02, -7.1247e-03,  1.5522e-02,  7.1313e-02,  1.0342e-01,\n",
       "          9.4814e-06,  4.3368e-02,  6.7315e-02,  6.6834e-02, -2.3287e-01,\n",
       "         -2.9619e-03,  1.0511e-01,  2.7963e-03, -3.7127e-02,  5.1618e-02,\n",
       "          5.9880e-03,  5.6580e-02,  3.1119e-02,  1.1842e-01,  7.3939e-02,\n",
       "          8.0229e-02, -5.1396e-02, -5.6952e-02,  1.1683e-01,  4.2425e-03,\n",
       "          1.7112e-02, -2.7637e-03, -1.0890e-02,  1.6021e-01, -1.2583e-01,\n",
       "          8.4441e-02, -4.7473e-03, -3.2626e-02, -1.8554e-01, -5.2980e-02,\n",
       "          8.8764e-02, -6.8864e-02,  9.2975e-03, -1.3101e-01,  6.7715e-03,\n",
       "          8.4802e-02,  9.8584e-03, -5.5389e-02,  1.1852e-03,  5.3216e-02,\n",
       "          3.6560e-02, -3.3525e-02, -3.1920e-03, -7.3149e-04, -9.6105e-02,\n",
       "         -1.1541e-01, -2.1092e-02, -1.5818e-01,  2.0352e-02, -2.9003e-02,\n",
       "          1.0036e-01,  1.0614e-01,  5.7846e-02,  2.1447e-02, -9.7270e-02,\n",
       "         -4.7276e-02,  4.0026e-02,  3.8747e-02,  1.4870e-02, -2.5359e-02,\n",
       "         -5.1062e-02, -3.2935e-02,  4.7346e-03, -4.5877e-02,  6.9429e-02,\n",
       "          4.9239e-02,  1.7044e-01, -1.7863e-01, -1.3458e-02, -3.6861e-02,\n",
       "         -6.1412e-02,  1.1088e-01, -3.2604e-03,  1.1063e-01,  1.1556e-01,\n",
       "          4.4418e-02, -1.4347e-01,  2.0536e-02, -9.7114e-02,  1.7824e-01,\n",
       "         -2.4449e-02,  1.4262e-01,  8.1103e-02,  5.9719e-02, -7.5857e-02,\n",
       "         -9.7011e-02,  1.0859e-01, -6.8929e-04, -9.5534e-02, -1.5047e-01,\n",
       "          1.6022e-01,  3.3767e-02, -1.0103e-01,  1.1288e-01, -4.9287e-02,\n",
       "          2.3701e-02,  2.2363e-02, -8.3917e-03,  9.5933e-02,  8.7083e-03,\n",
       "         -3.2783e-02, -1.2214e-01,  6.4868e-02, -2.0274e-01, -1.1286e-01,\n",
       "         -8.0430e-02, -5.3814e-02, -4.1172e-02, -4.5424e-02,  2.4195e-01,\n",
       "          6.6060e-02,  7.8255e-02, -1.5791e-01,  8.4430e-02, -4.5492e-02,\n",
       "         -1.5118e-01,  2.4184e-02, -1.0583e-01, -4.7456e-02, -3.2066e-02,\n",
       "         -4.5756e-02,  1.1995e-01, -2.1109e-02, -7.2462e-02,  2.0882e-02,\n",
       "          1.4174e-03,  6.3285e-02, -8.8697e-02,  8.9866e-02,  8.2868e-02,\n",
       "          1.3593e-01,  7.0092e-02,  3.7476e-02,  6.0416e-02, -5.7924e-02,\n",
       "         -1.3180e-01,  6.0792e-02, -4.0198e-02, -5.0765e-02,  8.3741e-02,\n",
       "          1.1457e-01, -1.3086e-01, -1.5549e-02,  2.7229e-02,  9.5003e-02,\n",
       "          2.2262e-01,  7.6697e-02,  1.2174e-01,  2.9427e-02, -6.1780e-02,\n",
       "         -2.3284e-02, -4.3756e-02, -1.4964e-02,  6.3702e-03, -6.5598e-02,\n",
       "         -2.6849e-02,  2.6965e-02,  7.5990e-02, -6.2520e-02,  7.9410e-02,\n",
       "         -2.6523e-02,  3.6051e-02, -1.5753e-01, -6.8907e-02, -2.8070e-02,\n",
       "          5.2946e-02,  1.0242e-02, -6.9950e-02,  4.5123e-02, -5.6444e-02,\n",
       "          3.9515e-02, -4.9464e-02,  5.9210e-03,  6.8418e-02,  4.4863e-02,\n",
       "          8.5145e-02, -1.1847e-01,  3.2657e-02,  1.1503e-01,  9.5096e-02,\n",
       "          1.8622e-01, -6.7375e-02, -1.7129e-01,  2.4663e-02,  2.3287e-02,\n",
       "         -6.5713e-02, -1.0933e-01,  3.2050e-02,  1.8413e-02, -1.0868e-01,\n",
       "          1.5858e-03, -1.3237e-02,  7.6833e-02,  2.9519e-01, -7.6754e-02,\n",
       "         -1.0153e-02,  3.2983e-02, -1.0917e-01, -8.1519e-02,  1.2053e-02,\n",
       "          8.4874e-02,  1.4010e-02, -3.8045e-02,  3.3147e-02, -7.9660e-02,\n",
       "         -9.1574e-02, -1.6837e-02,  6.5300e-02, -1.5463e-01, -2.8345e-02,\n",
       "         -9.6493e-03,  3.9112e-03,  9.2773e-02, -9.0115e-02, -6.1527e-02,\n",
       "          1.5647e-01,  2.2132e-02, -1.7236e-02,  1.8942e-02, -4.4023e-02,\n",
       "         -8.5648e-02, -5.9910e-02, -2.0610e-02,  7.9054e-02,  2.8705e-02,\n",
       "          7.8924e-02, -1.3408e-01,  8.9694e-03, -1.3905e-02,  1.5762e-01,\n",
       "         -5.0140e-03,  7.9625e-02, -4.8758e-02, -4.5016e-02,  2.2739e-02,\n",
       "         -1.8808e-02,  1.5132e-01, -1.7795e-01,  2.0922e-02, -6.4212e-02,\n",
       "          5.0832e-02,  1.9901e-02,  1.3213e-01,  3.7426e-02,  9.5119e-02,\n",
       "          6.8872e-02,  1.6680e-01, -7.2225e-02, -6.6644e-02,  8.8258e-03,\n",
       "         -5.5869e-02, -5.3547e-02, -5.4783e-02,  1.6293e-01,  1.9823e-01,\n",
       "          2.6007e-02,  1.8276e-02,  4.1339e-02, -1.1173e-01,  6.7223e-02,\n",
       "         -9.4768e-02, -7.2545e-02, -4.8898e-02,  8.2448e-02,  1.1437e-01,\n",
       "          3.3942e-02,  2.4120e-01, -8.7061e-02,  2.4377e-02,  4.8723e-02,\n",
       "         -4.9315e-02, -1.1814e-01,  3.8711e-02,  1.1499e-02,  1.1471e-01,\n",
       "         -1.1456e-01,  1.7458e-02, -2.4040e-02,  8.0559e-02,  1.3795e-01,\n",
       "         -3.4642e-02, -6.6790e-02, -6.0768e-02, -1.7066e-01, -3.0745e-02,\n",
       "          1.1279e-02,  3.5547e-02,  3.3099e-02, -6.4488e-04, -5.2463e-02,\n",
       "         -1.8767e-01,  2.2300e-02,  3.3776e-02,  8.6301e-02,  4.5322e-02,\n",
       "         -7.0989e-02,  8.0053e-02, -8.9165e-02, -6.2591e-03, -5.2264e-02,\n",
       "          4.6569e-02, -1.1729e-01,  1.1060e-02,  6.0386e-02, -1.2753e-01,\n",
       "          7.2824e-02,  2.5105e-02,  1.2054e-01,  1.2737e-01, -9.0979e-02,\n",
       "          1.1402e-01,  3.8291e-02, -2.6255e-01,  6.4277e-02, -6.6120e-02,\n",
       "         -1.1498e-01,  3.0123e-02, -5.0036e-03,  9.0720e-02, -1.1868e-01,\n",
       "          9.8538e-02, -3.9093e-02, -3.0241e-01, -1.4491e-02,  1.4235e-04,\n",
       "         -1.2438e-01,  1.1921e-01, -5.3509e-02, -5.5835e-02,  1.9969e-02,\n",
       "          6.8141e-02, -4.8850e-03,  3.3746e-02,  6.3847e-02, -2.4482e-02,\n",
       "          2.0283e-02, -4.4306e-03,  7.1042e-02, -5.7392e-02, -1.5600e-02,\n",
       "          1.1312e-01, -8.6242e-03, -5.6188e-02,  1.4732e-01,  1.9270e-01,\n",
       "         -6.9593e-03,  5.1571e-02,  1.3901e-01, -5.4643e-02, -7.5529e-02,\n",
       "         -1.0174e-01, -2.7704e-02,  6.7120e-02, -3.5643e-02,  1.1774e-02,\n",
       "          6.6404e-02, -3.5015e-02, -4.9473e-02, -4.3960e-02,  2.8481e-02,\n",
       "         -2.1269e-01,  5.6303e-02, -2.8185e-02,  4.5498e-02,  8.8562e-02,\n",
       "         -1.8828e-02,  3.0364e-03, -1.0445e-02, -2.1055e-01, -2.5681e-02,\n",
       "          4.9384e-02, -4.6242e-02,  7.4544e-04,  1.1827e-01, -3.1030e-02,\n",
       "          4.1525e-02, -5.8566e-02,  2.6768e-03, -7.6966e-02,  3.8111e-02,\n",
       "         -3.0220e-02,  2.3184e-01,  1.3778e-01, -3.4511e-02,  1.8831e-02,\n",
       "         -3.3153e-02, -5.2012e-03, -2.1462e-02,  3.4086e-02,  3.1244e-02,\n",
       "          9.9397e-02,  7.6571e-02, -5.6897e-02, -1.8559e-01,  4.0658e-02,\n",
       "         -1.1131e-01, -1.9881e-02,  1.3019e-02,  9.0915e-02,  1.0551e-01,\n",
       "         -1.5419e-32,  4.4289e-02, -1.6660e-01,  1.1265e-01,  1.1086e-01,\n",
       "         -1.4311e-02,  1.1198e-02, -1.0322e-01,  1.8364e-02, -3.2733e-02,\n",
       "         -3.2270e-02,  8.2752e-03, -2.0653e-03,  2.1874e-04,  6.6688e-02,\n",
       "          9.9612e-02,  2.7419e-02, -1.0925e-01, -8.7036e-02,  2.0399e-03,\n",
       "          2.8134e-02, -6.5512e-02,  3.6749e-02, -5.5335e-03,  4.1112e-03,\n",
       "          1.2443e-02,  8.6269e-02, -9.7593e-02,  7.4146e-02, -7.9152e-02,\n",
       "         -2.4367e-02, -5.1913e-02, -1.0982e-01, -4.7026e-02,  1.7632e-01,\n",
       "          1.1406e-02, -1.9663e-01,  5.9274e-02, -3.2468e-03,  1.8188e-02,\n",
       "          7.9948e-02,  1.2169e-01, -4.6733e-02,  7.7958e-02,  7.7325e-02,\n",
       "          4.5716e-02, -5.5100e-02, -9.8063e-02,  1.9418e-02, -6.8467e-02,\n",
       "         -1.2982e-01,  1.3832e-01,  2.1683e-02, -1.7057e-02, -8.9812e-02,\n",
       "          8.8611e-02, -2.1594e-03, -5.4770e-02, -1.4723e-01, -4.2444e-02,\n",
       "          3.5240e-02,  7.3708e-02, -7.1264e-02, -8.5809e-02,  6.2210e-02,\n",
       "         -4.0763e-03, -8.9969e-02,  2.2704e-01, -9.4268e-02,  1.1436e-01,\n",
       "         -5.3452e-02,  2.1572e-02,  2.2260e-02,  4.5886e-03,  8.6425e-02,\n",
       "          9.3499e-03,  4.7880e-02, -1.0689e-01,  1.4661e-02, -4.3936e-02,\n",
       "          1.4772e-02, -1.1625e-01, -1.2030e-02,  1.3018e-01, -1.5181e-02,\n",
       "         -4.0662e-02, -3.9971e-02, -5.7904e-02,  1.7654e-01, -4.5774e-02,\n",
       "         -4.7946e-02,  1.1959e-01,  2.5628e-02, -6.6189e-02,  8.5911e-02,\n",
       "         -6.3396e-02, -1.3258e-02, -1.6233e-01,  1.0034e-01,  1.0969e-01,\n",
       "         -7.1125e-02, -6.6409e-02,  1.0218e-01, -1.6438e-01, -1.7278e-02,\n",
       "         -1.2227e-02,  1.4849e-02,  7.3652e-02,  2.8524e-02, -3.5444e-02,\n",
       "          3.8554e-02,  4.1547e-02,  2.1731e-02,  4.5273e-02, -6.0037e-02,\n",
       "         -1.6400e-03,  2.3711e-02,  3.4196e-02,  9.0026e-02,  3.4212e-02,\n",
       "         -2.9284e-02, -3.6985e-02, -3.0914e-01,  2.1663e-01,  1.4767e-01,\n",
       "         -9.5780e-02,  2.4215e-02,  1.0077e-01,  3.5559e-01, -3.5944e-02,\n",
       "          4.2322e-02,  2.1426e-02, -1.0355e-02,  7.3772e-07,  8.7103e-02,\n",
       "         -6.7409e-03, -1.1913e-01, -1.2855e-01, -5.2969e-03,  4.2242e-03,\n",
       "          5.0357e-03,  4.1485e-02,  2.4039e-03,  1.0127e-01,  1.1156e-01,\n",
       "          1.2978e-02, -2.3430e-02, -1.0651e-01, -6.7476e-02, -4.0151e-02,\n",
       "          4.2615e-02, -9.0598e-02, -1.0587e-01, -1.2516e-02, -1.9116e-01,\n",
       "         -4.0895e-02,  6.0272e-02, -1.2241e-01,  8.6672e-03,  1.7020e-02,\n",
       "          8.6372e-02,  7.1403e-02, -9.5271e-02, -1.5028e-01,  1.1807e-01,\n",
       "         -1.1035e-01, -1.8430e-01,  5.9283e-03,  2.1165e-02,  2.2055e-03,\n",
       "         -7.9048e-02,  5.8308e-02, -3.3503e-02,  2.4994e-01,  3.7825e-02,\n",
       "         -5.0644e-02, -1.0968e-01, -3.7491e-02,  8.7927e-02,  4.2905e-02,\n",
       "          6.8359e-02,  1.5009e-02, -2.9435e-02,  1.2415e-02,  1.0382e-01,\n",
       "         -2.3641e-02, -8.6078e-02,  4.2851e-02,  3.5536e-03, -3.8858e-02,\n",
       "          2.0054e-02,  5.8564e-02,  1.1125e-01,  5.5960e-03, -4.1410e-02,\n",
       "         -1.3621e-02, -8.4206e-02,  1.1787e-02,  8.0206e-02,  8.8670e-02,\n",
       "          2.8307e-02,  6.6963e-34, -1.6847e-01,  4.7647e-02,  1.9819e-02,\n",
       "         -4.6766e-02, -5.8538e-02, -8.0642e-02, -1.6062e-01, -8.9962e-03,\n",
       "          7.0725e-02,  5.8819e-02, -4.0935e-02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = torch.tensor([[ 0.0532, -0.0455,  0.0135, -0.0191,  0.0344,  0.0193,  0.0134, -0.0000,\n",
    "         -0.0134, -0.0046,  0.1211,  0.0063,  0.0393,  0.0669,  0.0254,  0.0316,\n",
    "         -0.0398,  0.0168,  0.0690,  0.0143,  0.0618, -0.0227, -0.0034, -0.0000,\n",
    "         -0.0643,  0.0196,  0.0000, -0.0250,  0.0550,  0.0208, -0.0000, -0.0274,\n",
    "          0.0000, -0.0078,  0.0512,  0.0045, -0.0218,  0.0327,  0.0623,  0.0624,\n",
    "         -0.0184, -0.0210,  0.0130,  0.0391, -0.0050, -0.0210,  0.0167, -0.0490]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(embeddings):\n",
    "    \"\"\"Returned the flattened upper triangular cosine similarity matrix of the given embeddings.\"\"\"\n",
    "    cos_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "    similarity_matrix = cos_sim(embeddings.unsqueeze(0), embeddings.unsqueeze(1))\n",
    "    indices = torch.triu_indices(*similarity_matrix.shape, offset=1)\n",
    "    return similarity_matrix[indices[0], indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2964],\n",
       "        [0.2964, 1.0000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_red_test = torch.cat((X_red, torch.rand(X_red.shape)), dim=0)\n",
    "print(X_red_test.shape)\n",
    "# test1 = torch.tensor([1., 3.2, -.6])\n",
    "# print(test1.unsqueeze(0))\n",
    "# print(test1.unsqueeze(1))\n",
    "torch.nn.CosineSimilarity(dim=-1, eps=1e-6)(X_red_test.unsqueeze(0), X_red_test.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2964])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cos_sim(X_red_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "from reduced_encoders.modeling_utils import compressed_contrastive_loss\n",
    "\n",
    "compressed_contrastive_loss(X, X_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cayjobla/miniconda3/envs/reduced_encoders/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/cayjobla/ReducedEncoders/reduced_encoders/configuration_reduced.py:25: UserWarning: This config has both 'can_reduce_sequence' and 'can_reduce_pooled' set to False. Models using this config will not apply reduction on forward pass\n",
      "  warnings.warn(\"This config has both 'can_reduce_sequence' and \"\n",
      "Some weights of MPNetReducedForSequenceClassification were not initialized from the model checkpoint at cayjobla/all-mpnet-base-v2-reduced and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import reduced_encoders\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "model_checkpoint = \"cayjobla/all-mpnet-base-v2-reduced\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "config.can_reduce_pooled = True\n",
    "config.can_reduce_sequence = True\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetReducedConfig {\n",
       "  \"_name_or_path\": \"cayjobla/all-mpnet-base-v2-reduced\",\n",
       "  \"architectures\": [\n",
       "    \"MPNetReducedModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"can_reduce_pooled\": true,\n",
       "  \"can_reduce_sequence\": true,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"mpnet_reduced\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooling_mode\": \"mean\",\n",
       "  \"reduced_size\": 48,\n",
       "  \"reduction_sizes\": [\n",
       "    512,\n",
       "    256,\n",
       "    128,\n",
       "    64,\n",
       "    48\n",
       "  ],\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"vocab_size\": 30527\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 2027, 2007, 1041, 3235, 6255, 1016,    2],\n",
       "        [   0, 2027, 2007, 2182, 3235, 6255, 1016,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"This is a test sentence.\", \"This is another test sentence.\"]\n",
    "test_input = tokenizer(test_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1012, -0.1223, -0.1231,  ...,  0.0111,  0.0648, -0.0549],\n",
      "         [-0.0282, -0.1746, -0.0488,  ..., -0.0215, -0.0088, -0.0277],\n",
      "         [-0.0371, -0.3194, -0.0771,  ..., -0.0313, -0.0640, -0.0381],\n",
      "         ...,\n",
      "         [-0.0275,  0.0332, -0.1447,  ...,  0.1002, -0.0054,  0.0410],\n",
      "         [ 0.1209, -0.0068, -0.1114,  ...,  0.0037,  0.0718, -0.0738],\n",
      "         [ 0.0921, -0.0739, -0.1236,  ...,  0.0031, -0.0017, -0.0497]],\n",
      "\n",
      "        [[ 0.0930, -0.1188, -0.1623,  ...,  0.0063,  0.0677, -0.0891],\n",
      "         [ 0.0157, -0.1210, -0.0872,  ...,  0.0292,  0.0640, -0.0467],\n",
      "         [ 0.0027, -0.2758, -0.1076,  ...,  0.0204,  0.0006, -0.0526],\n",
      "         ...,\n",
      "         [ 0.0166,  0.1070, -0.1878,  ...,  0.0867,  0.0138,  0.0298],\n",
      "         [ 0.1279, -0.0091, -0.1403,  ..., -0.0031,  0.0703, -0.1097],\n",
      "         [ 0.0893, -0.0701, -0.1546,  ...,  0.0082,  0.0011, -0.0756]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, attentions=None)\n",
      "tensor([[-0.0199, -0.0118, -0.0102,  0.0386, -0.0108,  0.0036, -0.0079,  0.0142,\n",
      "         -0.0398, -0.0155, -0.0054, -0.0012,  0.0052,  0.0115, -0.0215,  0.0148,\n",
      "         -0.0431, -0.0049, -0.0015,  0.0018,  0.0134, -0.0033, -0.0011, -0.0046,\n",
      "         -0.0130,  0.0055, -0.0099, -0.0040, -0.0003,  0.0110, -0.0133, -0.0038,\n",
      "         -0.0371,  0.0188, -0.0184, -0.0233, -0.0067,  0.0065,  0.0041,  0.0137,\n",
      "          0.0099,  0.0107, -0.0103,  0.0231, -0.0165,  0.0121,  0.0175, -0.0106],\n",
      "        [-0.0235, -0.0157, -0.0138,  0.0386, -0.0079,  0.0027, -0.0084,  0.0100,\n",
      "         -0.0371, -0.0152, -0.0073, -0.0078,  0.0042,  0.0105, -0.0227,  0.0161,\n",
      "         -0.0457, -0.0090, -0.0054,  0.0033,  0.0166, -0.0016,  0.0040, -0.0005,\n",
      "         -0.0222,  0.0017, -0.0089, -0.0045,  0.0024,  0.0162, -0.0150, -0.0039,\n",
      "         -0.0367,  0.0236, -0.0217, -0.0257, -0.0060,  0.0118, -0.0005,  0.0094,\n",
      "          0.0083,  0.0132, -0.0060,  0.0283, -0.0254,  0.0131,  0.0155, -0.0176]],\n",
      "       grad_fn=<GeluBackward0>)\n",
      "tensor([[-0.0199, -0.0118, -0.0102,  0.0386, -0.0108,  0.0036, -0.0079,  0.0142,\n",
      "         -0.0398, -0.0155, -0.0054, -0.0012,  0.0052,  0.0115, -0.0215,  0.0148,\n",
      "         -0.0431, -0.0049, -0.0015,  0.0018,  0.0134, -0.0033, -0.0011, -0.0046,\n",
      "         -0.0130,  0.0055, -0.0099, -0.0040, -0.0003,  0.0110, -0.0133, -0.0038,\n",
      "         -0.0371,  0.0188, -0.0184, -0.0233, -0.0067,  0.0065,  0.0041,  0.0137,\n",
      "          0.0099,  0.0107, -0.0103,  0.0231, -0.0165,  0.0121,  0.0175, -0.0106],\n",
      "        [-0.0235, -0.0157, -0.0138,  0.0386, -0.0079,  0.0027, -0.0084,  0.0100,\n",
      "         -0.0371, -0.0152, -0.0073, -0.0078,  0.0042,  0.0105, -0.0227,  0.0161,\n",
      "         -0.0457, -0.0090, -0.0054,  0.0033,  0.0166, -0.0016,  0.0040, -0.0005,\n",
      "         -0.0222,  0.0017, -0.0089, -0.0045,  0.0024,  0.0162, -0.0150, -0.0039,\n",
      "         -0.0367,  0.0236, -0.0217, -0.0257, -0.0060,  0.0118, -0.0005,  0.0094,\n",
      "          0.0083,  0.0132, -0.0060,  0.0283, -0.0254,  0.0131,  0.0155, -0.0176]],\n",
      "       grad_fn=<GeluBackward0>)\n",
      "tensor([[0.0009, 0.0038],\n",
      "        [0.0008, 0.0044]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[0.0009, 0.0038],\n",
       "        [0.0008, 0.0044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = model(**test_input)\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0262, -0.0025, -0.0434, -0.0015, -0.0233, -0.0042, -0.0117, -0.0164,\n",
       "          0.0343, -0.0091,  0.0011,  0.0057,  0.0422, -0.0208,  0.0064,  0.0275,\n",
       "          0.0253, -0.0075, -0.0088, -0.0333, -0.0076,  0.0004, -0.0222, -0.0035,\n",
       "          0.0093,  0.0301, -0.0030,  0.0028, -0.0039,  0.0171, -0.0083, -0.0093,\n",
       "         -0.0522,  0.0065, -0.0110, -0.0120, -0.0246, -0.0023, -0.0090,  0.0206,\n",
       "         -0.0100,  0.0348, -0.0194, -0.0037, -0.0213, -0.0369, -0.0147,  0.0251],\n",
       "        [-0.0148, -0.0332, -0.0134, -0.0449, -0.0239,  0.0095,  0.0314,  0.0167,\n",
       "         -0.0049, -0.0075, -0.0379, -0.0220, -0.0073, -0.0141,  0.0083, -0.0027,\n",
       "         -0.0199, -0.0043,  0.0312,  0.0248, -0.0131, -0.0005,  0.0466,  0.0176,\n",
       "         -0.0081,  0.0232, -0.0312, -0.0032, -0.0047, -0.0171, -0.0124, -0.0176,\n",
       "         -0.0281,  0.0007, -0.0125, -0.0122,  0.0101, -0.0028,  0.0274,  0.0208,\n",
       "          0.0039,  0.0195, -0.0148, -0.0035, -0.0477,  0.0077,  0.0065,  0.0030]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I am getting 'nan' or 'inf' values for the gradient norm, most likely \n",
    "#       because output logits are extremely large or 'nan' values themselves.\n",
    "#       I suspect it was either a result of changing model structure or because\n",
    "#       I updated my version of huggingface, which slightly altered how model\n",
    "#       checkpoints are loaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
