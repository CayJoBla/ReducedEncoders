You try to use a model that was created with version 3.5.0.dev0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.



/home/cayjobla/miniconda3/envs/reduced_encoders/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
wandb: Currently logged in as: cayjobla. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/cayjobla/ReducedEncoders/wandb/run-20250321_220847-0s94g16c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neobert-reduced
wandb: â­ï¸ View project at https://wandb.ai/cayjobla/reduced-encoders
wandb: ğŸš€ View run at https://wandb.ai/cayjobla/reduced-encoders/runs/0s94g16c
Loaded model: tomaarsen/NeoBERT-gooaq-8e-05
Initialized reduction and expansion layers with sizes: [768, 512, 256, 128, 64, 48]
Loaded dataset: sentence-transformers/gooaq, split: train[:5%], eval size: 5000
Initialized DimensionalReductionLoss with weights: tensor([0.5000, 0.5000])
Initialized optimizer and LR scheduler
Initialized accelerator
Starting training loop...
Epoch: 0.0, Step: 0, Eval Loss: 8.20373862138469
Epoch: 0.10300358452474147, Step: 2500, Train Loss: 5.259518146514893
Epoch: 0.20600716904948294, Step: 5000, Train Loss: 3.644988775253296
Epoch: 0.30901075357422436, Step: 7500, Train Loss: 2.1562905311584473
Epoch: 0.41201433809896587, Step: 10000, Train Loss: 1.6442689895629883
Epoch: 0.5150179226237073, Step: 12500, Train Loss: 1.2622658014297485
Epoch: 0.6180215071484487, Step: 15000, Train Loss: 1.0481714010238647
Epoch: 0.7210250916731902, Step: 17500, Train Loss: 1.0078964233398438
Epoch: 0.8240286761979317, Step: 20000, Train Loss: 1.1482632160186768
Epoch: 0.9270322607226732, Step: 22500, Train Loss: 0.6788117289543152
Epoch: 1.0, Step: 24271, Eval Loss: 0.5979176523874132
Epoch: 1.0300358452474145, Step: 25000, Train Loss: 0.6438616514205933
Epoch: 1.133039429772156, Step: 27500, Train Loss: 0.698499858379364
Epoch: 1.2360430142968974, Step: 30000, Train Loss: 0.45155155658721924
Epoch: 1.339046598821639, Step: 32500, Train Loss: 0.5418893694877625
Epoch: 1.4420501833463804, Step: 35000, Train Loss: 0.46038901805877686
Epoch: 1.5450537678711218, Step: 37500, Train Loss: 0.4066126346588135
Epoch: 1.6480573523958635, Step: 40000, Train Loss: 0.26527589559555054
Epoch: 1.751060936920605, Step: 42500, Train Loss: 0.3742624521255493
Epoch: 1.8540645214453464, Step: 45000, Train Loss: 0.4749300479888916
Epoch: 1.9570681059700878, Step: 47500, Train Loss: 0.623282790184021
Epoch: 2.0, Step: 48542, Eval Loss: 0.35633429749120626
Epoch: 2.060071690494829, Step: 50000, Train Loss: 0.6134093999862671
Epoch: 2.1630752750195708, Step: 52500, Train Loss: 0.7658265233039856
Epoch: 2.266078859544312, Step: 55000, Train Loss: 0.583783745765686
Epoch: 2.3690824440690537, Step: 57500, Train Loss: 0.4366188645362854
Epoch: 2.472086028593795, Step: 60000, Train Loss: 0.49479830265045166
Epoch: 2.5750896131185366, Step: 62500, Train Loss: 0.35565996170043945
Epoch: 2.678093197643278, Step: 65000, Train Loss: 0.1858431100845337
Epoch: 2.7810967821680195, Step: 67500, Train Loss: 0.30569958686828613
Epoch: 2.8841003666927607, Step: 70000, Train Loss: 0.34144914150238037
Epoch: 2.9871039512175024, Step: 72500, Train Loss: 0.4562843441963196
Epoch: 3.0, Step: 72813, Eval Loss: 0.3212561197846914
wandb: - 0.014 MB of 0.014 MB uploadedwandb: \ 0.014 MB of 0.014 MB uploadedwandb: | 0.028 MB of 0.028 MB uploadedwandb: 
wandb: Run history:
wandb:        eval/cosine_sim_loss â–ˆâ–â–â–
wandb:                   eval/loss â–ˆâ–â–â–
wandb:    eval/reconstruction_loss â–ˆâ–„â–‚â–
wandb:       train/cosine_sim_loss â–‡â–†â–ƒâ–…â–ƒâ–„â–…â–ˆâ–ƒâ–„â–…â–ƒâ–„â–ƒâ–ƒâ–â–ƒâ–ƒâ–†â–…â–‡â–…â–„â–„â–‚â–â–‚â–‚â–ƒ
wandb:     train/cosine_sim_weight â–â–â–„â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 train/epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           train/global_step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         train/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–
wandb:                  train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   train/reconstruction_loss â–‡â–‡â–„â–„â–†â–ƒâ–…â–…â–ˆâ–„â–…â–â–„â–ƒâ–ƒâ–„â–„â–‡â–‚â–ƒâ–ƒâ–„â–‚â–„â–†â–‚â–…â–…â–…
wandb: train/reconstruction_weight â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:        eval/cosine_sim_loss 0.01753
wandb:                   eval/loss 0.32126
wandb:    eval/reconstruction_loss 14.51998
wandb:       train/cosine_sim_loss 0.0219
wandb:     train/cosine_sim_weight 24.5874
wandb:                 train/epoch 3.0
wandb:           train/global_step 72813
wandb:         train/learning_rate 0.0
wandb:                  train/loss 0.45628
wandb:   train/reconstruction_loss 15.33374
wandb: train/reconstruction_weight 0.03387
wandb: 
wandb: ğŸš€ View run neobert-reduced at: https://wandb.ai/cayjobla/reduced-encoders/runs/0s94g16c
wandb: â­ï¸ View project at: https://wandb.ai/cayjobla/reduced-encoders
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250321_220847-0s94g16c/logs
