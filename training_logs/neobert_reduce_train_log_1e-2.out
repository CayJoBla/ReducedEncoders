You try to use a model that was created with version 3.5.0.dev0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.



/home/cayjobla/miniconda3/envs/reduced_encoders/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
wandb: Currently logged in as: cayjobla. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/cayjobla/ReducedEncoders/wandb/run-20250321_220819-8jzoto9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neobert-reduced
wandb: ⭐️ View project at https://wandb.ai/cayjobla/reduced-encoders
wandb: 🚀 View run at https://wandb.ai/cayjobla/reduced-encoders/runs/8jzoto9u
Loaded model: tomaarsen/NeoBERT-gooaq-8e-05
Initialized reduction and expansion layers with sizes: [768, 512, 256, 128, 64, 48]
Loaded dataset: sentence-transformers/gooaq, split: train[:5%], eval size: 5000
Initialized DimensionalReductionLoss with weights: tensor([0.5000, 0.5000])
Initialized optimizer and LR scheduler
Initialized accelerator
Starting training loop...
Epoch: 0.0, Step: 0, Eval Loss: 8.209663361954174
Epoch: 0.10300358452474147, Step: 2500, Train Loss: 0.9677654504776001
Epoch: 0.20600716904948294, Step: 5000, Train Loss: 1.0210763216018677
Epoch: 0.30901075357422436, Step: 7500, Train Loss: 1.0168836116790771
Epoch: 0.41201433809896587, Step: 10000, Train Loss: 1.1122236251831055
Epoch: 0.5150179226237073, Step: 12500, Train Loss: 1.4977413415908813
Epoch: 0.6180215071484487, Step: 15000, Train Loss: 1.9405124187469482
Epoch: 0.7210250916731902, Step: 17500, Train Loss: 2.779289484024048
Epoch: 0.8240286761979317, Step: 20000, Train Loss: 2.3201475143432617
Epoch: 0.9270322607226732, Step: 22500, Train Loss: 2.1028976440429688
Epoch: 1.0, Step: 24271, Eval Loss: 1.665912288555996
Epoch: 1.0300358452474145, Step: 25000, Train Loss: 1.8209502696990967
Epoch: 1.133039429772156, Step: 27500, Train Loss: 1.9276305437088013
Epoch: 1.2360430142968974, Step: 30000, Train Loss: 2.1731529235839844
Epoch: 1.339046598821639, Step: 32500, Train Loss: 2.4511325359344482
Epoch: 1.4420501833463804, Step: 35000, Train Loss: 3.294921875
Epoch: 1.5450537678711218, Step: 37500, Train Loss: 3.327709674835205
Epoch: 1.6480573523958635, Step: 40000, Train Loss: 3.9259705543518066
Epoch: 1.751060936920605, Step: 42500, Train Loss: 4.963380813598633
Epoch: 1.8540645214453464, Step: 45000, Train Loss: 3.9528627395629883
Epoch: 1.9570681059700878, Step: 47500, Train Loss: 2.6653618812561035
Epoch: 2.0, Step: 48542, Eval Loss: 2.2089089358167397
Epoch: 2.060071690494829, Step: 50000, Train Loss: 1.6684622764587402
Epoch: 2.1630752750195708, Step: 52500, Train Loss: 1.170442819595337
Epoch: 2.266078859544312, Step: 55000, Train Loss: 1.07374906539917
Epoch: 2.3690824440690537, Step: 57500, Train Loss: 0.9879626631736755
Epoch: 2.472086028593795, Step: 60000, Train Loss: 0.9621630907058716
Epoch: 2.5750896131185366, Step: 62500, Train Loss: 0.9578478336334229
Epoch: 2.678093197643278, Step: 65000, Train Loss: 0.9156691431999207
Epoch: 2.7810967821680195, Step: 67500, Train Loss: 0.8671618103981018
Epoch: 2.8841003666927607, Step: 70000, Train Loss: 0.9837585687637329
Epoch: 2.9871039512175024, Step: 72500, Train Loss: 0.8737813234329224
Epoch: 3.0, Step: 72813, Eval Loss: 0.9720682436756665
wandb: - 0.014 MB of 0.014 MB uploadedwandb: \ 0.020 MB of 0.030 MB uploaded (0.004 MB deduped)wandb: | 0.034 MB of 0.034 MB uploaded (0.004 MB deduped)wandb: 
wandb: Run history:
wandb:        eval/cosine_sim_loss █▂▁▁
wandb:                   eval/loss █▂▂▁
wandb:    eval/reconstruction_loss █▁▁▁
wandb:       train/cosine_sim_loss ▁▂▂▂▁▂▅▃▆█▆▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁
wandb:     train/cosine_sim_weight ▆█▇█▇▆▁▇█▃▁▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇
wandb:                 train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:           train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:         train/learning_rate ███▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▁▁▁
wandb:                  train/loss ▁▁▁▁▂▃▄▃▃▃▃▃▄▅▅▆█▆▄▂▂▁▁▁▁▁▁▁▁
wandb:   train/reconstruction_loss ▅▄▄▄▇▃▄▆▅▄▂▄▃▅▅▃█▄▄▄▃▆▅▄▄▃▂▆▁
wandb: train/reconstruction_weight ▂▁▁▂▃▄▄▄▃▂▃▄▄▆▆▇█▇▅▃▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:        eval/cosine_sim_loss 0.06047
wandb:                   eval/loss 0.97207
wandb:    eval/reconstruction_loss 15.63888
wandb:       train/cosine_sim_loss 0.05287
wandb:     train/cosine_sim_weight 8.38047
wandb:                 train/epoch 3.0
wandb:           train/global_step 72813
wandb:         train/learning_rate 4e-05
wandb:                  train/loss 0.87378
wandb:   train/reconstruction_loss 14.55793
wandb: train/reconstruction_weight 0.03209
wandb: 
wandb: 🚀 View run neobert-reduced at: https://wandb.ai/cayjobla/reduced-encoders/runs/8jzoto9u
wandb: ⭐️ View project at: https://wandb.ai/cayjobla/reduced-encoders
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250321_220819-8jzoto9u/logs
train_neobert.sh: line 4: py: command not found
