[rank1]:[W325 16:03:53.464871750 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W325 16:03:54.483697013 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.65s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.54s/it]
[rank0]:[W325 16:04:12.666261379 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.65s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.69s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.75s/it]
Loaded model: Linq-AI-Research/Linq-Embed-Mistral
Initialized reduction and expansion layers with sizes: [4096, 3584, 2048, 1536, 1024, 768, 512, 384, 256, 128, 64]
Loaded medi dataset: /home/cayjobla/ReducedEncoders/medi-data/medi-data.json, eval size: 8000
Initialized DimensionalReductionLoss with weights: tensor([0.5000, 0.5000])
Initialized optimizer and LR scheduler
wandb: Currently logged in as: cayjobla. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/cayjobla/ReducedEncoders/wandb/run-20250325_160522-jrbr7nh4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mistral-reduced
wandb: ⭐️ View project at https://wandb.ai/cayjobla/reduced-encoders
wandb: 🚀 View run at https://wandb.ai/cayjobla/reduced-encoders/runs/jrbr7nh4
Initialized wandb tracking with run name: mistral-reduced
Starting training loop...
Running evaluation (step 0)...
Epoch: 0.0, Step: 0, Eval Loss: 2.919329881668091
Epoch: 0.0, Step: 0, Train Loss: 3.028665542602539
Running evaluation (step 84)...
Epoch: 1.0, Step: 84, Eval Loss: 2.7086422443389893
wandb: - 0.014 MB of 0.026 MB uploadedwandb: 
wandb: Run history:
wandb:        eval/cosine_sim_loss █▁
wandb:                   eval/loss █▁
wandb:    eval/reconstruction_loss █▁
wandb:       train/cosine_sim_loss ▁
wandb:     train/cosine_sim_weight ▁
wandb:                 train/epoch ▁▁█
wandb:           train/global_step ▁▁█
wandb:         train/learning_rate ▁
wandb:                  train/loss ▁
wandb:   train/reconstruction_loss ▁
wandb: train/reconstruction_weight ▁
wandb: 
wandb: Run summary:
wandb:        eval/cosine_sim_loss 0.56495
wandb:                   eval/loss 2.70864
wandb:    eval/reconstruction_loss 3.49052
wandb:       train/cosine_sim_loss 0.42226
wandb:     train/cosine_sim_weight 0.5001
wandb:                 train/epoch 1.0
wandb:           train/global_step 84
wandb:         train/learning_rate 0.0001
wandb:                  train/loss 3.02867
wandb:   train/reconstruction_loss 4.24878
wandb: train/reconstruction_weight 0.4999
wandb: 
wandb: 🚀 View run mistral-reduced at: https://wandb.ai/cayjobla/reduced-encoders/runs/jrbr7nh4
wandb: ⭐️ View project at: https://wandb.ai/cayjobla/reduced-encoders
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250325_160522-jrbr7nh4/logs
[rank0]:[W325 16:17:35.225628972 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
