You try to use a model that was created with version 3.5.0.dev0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.



/home/cayjobla/miniconda3/envs/reduced_encoders/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
wandb: Currently logged in as: cayjobla. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/cayjobla/ReducedEncoders/wandb/run-20250327_163649-y4svqdzu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neobert-reduced
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cayjobla/reduced-encoders
wandb: üöÄ View run at https://wandb.ai/cayjobla/reduced-encoders/runs/y4svqdzu
Loaded model: tomaarsen/NeoBERT-gooaq-8e-05
Initialized reduction and expansion layers with sizes: [768, 512, 256, 128, 64, 48]
Loaded dataset: sentence-transformers/gooaq, split: train, eval size: 5000
Initialized DimensionalReductionLoss with weights: tensor([0.5000, 0.5000])
Initialized optimizer and LR scheduler
Initialized wandb tracking with run name: neobert-reduced
Starting training loop...
Running evaluation (step 0)...
Epoch: 0.0, Step: 0, Eval Loss: 8.874702453613281
Epoch: 0.0, Step: 0, Train Loss: 8.788969039916992
Epoch: 0.004987531172069825, Step: 2500, Train Loss: 6.612885843701765
Epoch: 0.00997506234413965, Step: 5000, Train Loss: 4.959918808732073
Epoch: 0.014962593516209476, Step: 7500, Train Loss: 4.04519283293152
Epoch: 0.0199501246882793, Step: 10000, Train Loss: 3.5647637386367794
Epoch: 0.02493765586034913, Step: 12500, Train Loss: 3.260863591247478
Epoch: 0.029925187032418952, Step: 15000, Train Loss: 3.0473946967718404
Epoch: 0.034912718204488775, Step: 17500, Train Loss: 2.886638432916563
Epoch: 0.0399002493765586, Step: 20000, Train Loss: 2.759630644204026
Epoch: 0.04488778054862843, Step: 22500, Train Loss: 2.6557273815951925
Epoch: 0.04987531172069826, Step: 25000, Train Loss: 2.56844013896705
Epoch: 0.05486284289276808, Step: 27500, Train Loss: 2.493716452599508
Epoch: 0.059850374064837904, Step: 30000, Train Loss: 2.4286377941591373
Epoch: 0.06483790523690773, Step: 32500, Train Loss: 2.370946713566527
Epoch: 0.06982543640897755, Step: 35000, Train Loss: 2.319423848930582
Epoch: 0.07481296758104738, Step: 37500, Train Loss: 2.272974805426163
Epoch: 0.0798004987531172, Step: 40000, Train Loss: 2.2307235969608685
Epoch: 0.08478802992518704, Step: 42500, Train Loss: 2.1920926037857034
Epoch: 0.08977556109725686, Step: 45000, Train Loss: 2.1564722536844703
Epoch: 0.09476309226932668, Step: 47500, Train Loss: 2.123562644462616
Running evaluation (step 50000)...
Epoch: 0.09975062344139651, Step: 50000, Eval Loss: 1.4783798456192017
Epoch: 0.09975062344139651, Step: 50000, Train Loss: 2.0929591408567605
Epoch: 0.10473815461346633, Step: 52500, Train Loss: 2.064418259803387
Epoch: 0.10972568578553615, Step: 55000, Train Loss: 2.0376507421920143
Epoch: 0.11471321695760599, Step: 57500, Train Loss: 2.012511180955653
Epoch: 0.11970074812967581, Step: 60000, Train Loss: 1.9888141659469942
Epoch: 0.12468827930174564, Step: 62500, Train Loss: 1.9664393328462237
Epoch: 0.12967581047381546, Step: 65000, Train Loss: 1.945196407136289
Epoch: 0.13466334164588528, Step: 67500, Train Loss: 1.925062478979408
Epoch: 0.1396508728179551, Step: 70000, Train Loss: 1.9058860285047363
Epoch: 0.14463840399002495, Step: 72500, Train Loss: 1.887586012432288
Epoch: 0.14962593516209477, Step: 75000, Train Loss: 1.870133783246334
Epoch: 0.1546134663341646, Step: 77500, Train Loss: 1.8534301817336392
Epoch: 0.1596009975062344, Step: 80000, Train Loss: 1.8374621021236635
Epoch: 0.16458852867830423, Step: 82500, Train Loss: 1.822076917337641
Epoch: 0.16957605985037408, Step: 85000, Train Loss: 1.8073411241696682
Epoch: 0.1745635910224439, Step: 87500, Train Loss: 1.793139018337639
Epoch: 0.17955112219451372, Step: 90000, Train Loss: 1.7794978308919533
Epoch: 0.18453865336658354, Step: 92500, Train Loss: 1.7663745468801357
Epoch: 0.18952618453865336, Step: 95000, Train Loss: 1.7537451830049915
Epoch: 0.19451371571072318, Step: 97500, Train Loss: 1.7415487881989393
Running evaluation (step 100000)...
Epoch: 0.19950124688279303, Step: 100000, Eval Loss: 1.2305638790130615
Epoch: 0.19950124688279303, Step: 100000, Train Loss: 1.7297500682458156
Epoch: 0.20448877805486285, Step: 102500, Train Loss: 1.7183210381652003
Epoch: 0.20947630922693267, Step: 105000, Train Loss: 1.7072920459387246
Epoch: 0.2144638403990025, Step: 107500, Train Loss: 1.6965637483272644
Epoch: 0.2194513715710723, Step: 110000, Train Loss: 1.686239961900457
Epoch: 0.22443890274314215, Step: 112500, Train Loss: 1.676199628786918
Epoch: 0.22942643391521197, Step: 115000, Train Loss: 1.6664617196817881
Epoch: 0.2344139650872818, Step: 117500, Train Loss: 1.6569947089052495
Epoch: 0.23940149625935161, Step: 120000, Train Loss: 1.6478227678395778
Epoch: 0.24438902743142144, Step: 122500, Train Loss: 1.6389010653655498
Epoch: 0.24937655860349128, Step: 125000, Train Loss: 1.6302706076440225
Epoch: 0.2543640897755611, Step: 127500, Train Loss: 1.6218216356044217
Epoch: 0.2593516209476309, Step: 130000, Train Loss: 1.61357199107875
Epoch: 0.26433915211970077, Step: 132500, Train Loss: 1.6055603503484552
Epoch: 0.26932668329177056, Step: 135000, Train Loss: 1.5977492942655422
Epoch: 0.2743142144638404, Step: 137500, Train Loss: 1.5901188166445592
Epoch: 0.2793017456359102, Step: 140000, Train Loss: 1.5826882902595374
Epoch: 0.28428927680798005, Step: 142500, Train Loss: 1.5754695434830346
Epoch: 0.2892768079800499, Step: 145000, Train Loss: 1.5684255002316125
Epoch: 0.2942643391521197, Step: 147500, Train Loss: 1.5615231384156185
Running evaluation (step 150000)...
Epoch: 0.29925187032418954, Step: 150000, Eval Loss: 1.101338267326355
Epoch: 0.29925187032418954, Step: 150000, Train Loss: 1.554775227028205
Epoch: 0.30423940149625933, Step: 152500, Train Loss: 1.548150388986761
Epoch: 0.3092269326683292, Step: 155000, Train Loss: 1.5416915462270566
Epoch: 0.314214463840399, Step: 157500, Train Loss: 1.5354249080066327
Epoch: 0.3192019950124688, Step: 160000, Train Loss: 1.529297267093294
Epoch: 0.32418952618453867, Step: 162500, Train Loss: 1.5232841609685612
Epoch: 0.32917705735660846, Step: 165000, Train Loss: 1.517331399543851
Epoch: 0.3341645885286783, Step: 167500, Train Loss: 1.5115448627092392
Epoch: 0.33915211970074816, Step: 170000, Train Loss: 1.5058918497338591
Epoch: 0.34413965087281795, Step: 172500, Train Loss: 1.5003565443575135
Epoch: 0.3491271820448878, Step: 175000, Train Loss: 1.4949344728130762
Epoch: 0.3541147132169576, Step: 177500, Train Loss: 1.4896724704558393
Epoch: 0.35910224438902744, Step: 180000, Train Loss: 1.4844797088474952
Epoch: 0.3640897755610973, Step: 182500, Train Loss: 1.479386631894415
Epoch: 0.3690773067331671, Step: 185000, Train Loss: 1.4743813254572367
Epoch: 0.3740648379052369, Step: 187500, Train Loss: 1.4694949688518832
Epoch: 0.3790523690773067, Step: 190000, Train Loss: 1.4646644342869264
Epoch: 0.38403990024937656, Step: 192500, Train Loss: 1.4599757438624341
Epoch: 0.38902743142144636, Step: 195000, Train Loss: 1.4553436089750797
Epoch: 0.3940149625935162, Step: 197500, Train Loss: 1.4507749694271843
Running evaluation (step 200000)...
Epoch: 0.39900249376558605, Step: 200000, Eval Loss: 1.0279501676559448
Epoch: 0.39900249376558605, Step: 200000, Train Loss: 1.4462714352502632
Epoch: 0.40399002493765584, Step: 202500, Train Loss: 1.44187707174499
Epoch: 0.4089775561097257, Step: 205000, Train Loss: 1.4375423566732095
Epoch: 0.4139650872817955, Step: 207500, Train Loss: 1.4332783545759664
Epoch: 0.41895261845386533, Step: 210000, Train Loss: 1.4291229926963254
Epoch: 0.4239401496259352, Step: 212500, Train Loss: 1.425055019982876
Epoch: 0.428927680798005, Step: 215000, Train Loss: 1.4210279904976726
Epoch: 0.4339152119700748, Step: 217500, Train Loss: 1.4170596150504455
Epoch: 0.4389027431421446, Step: 220000, Train Loss: 1.4131569801696597
Epoch: 0.44389027431421446, Step: 222500, Train Loss: 1.4092957329653837
Epoch: 0.4488778054862843, Step: 225000, Train Loss: 1.405493425452766
Epoch: 0.4538653366583541, Step: 227500, Train Loss: 1.4017639170298164
Epoch: 0.45885286783042395, Step: 230000, Train Loss: 1.3980855713332174
Epoch: 0.46384039900249374, Step: 232500, Train Loss: 1.3945104108640258
Epoch: 0.4688279301745636, Step: 235000, Train Loss: 1.3909678871099769
Epoch: 0.47381546134663344, Step: 237500, Train Loss: 1.3874490649083662
Epoch: 0.47880299251870323, Step: 240000, Train Loss: 1.383991919642593
Epoch: 0.4837905236907731, Step: 242500, Train Loss: 1.3806181156029802
Epoch: 0.48877805486284287, Step: 245000, Train Loss: 1.377300775648718
Epoch: 0.4937655860349127, Step: 247500, Train Loss: 1.3740387492876127
Running evaluation (step 250000)...
Epoch: 0.49875311720698257, Step: 250000, Eval Loss: 0.980116605758667
Epoch: 0.49875311720698257, Step: 250000, Train Loss: 1.3708171431316598
Epoch: 0.5037406483790524, Step: 252500, Train Loss: 1.3676295183557479
Epoch: 0.5087281795511222, Step: 255000, Train Loss: 1.3644709964713317
Epoch: 0.513715710723192, Step: 257500, Train Loss: 1.361388921666377
Epoch: 0.5187032418952618, Step: 260000, Train Loss: 1.3583205392085398
Epoch: 0.5236907730673317, Step: 262500, Train Loss: 1.3553207984325353
Epoch: 0.5286783042394015, Step: 265000, Train Loss: 1.3523479486688934
Epoch: 0.5336658354114713, Step: 267500, Train Loss: 1.3494143915727381
Epoch: 0.5386533665835411, Step: 270000, Train Loss: 1.3465107159428844
Epoch: 0.543640897755611, Step: 272500, Train Loss: 1.343652025591723
Epoch: 0.5486284289276808, Step: 275000, Train Loss: 1.3408619011205454
Epoch: 0.5536159600997507, Step: 277500, Train Loss: 1.3380587493841372
Epoch: 0.5586034912718204, Step: 280000, Train Loss: 1.3353370375959124
Epoch: 0.5635910224438903, Step: 282500, Train Loss: 1.3326057673585516
Epoch: 0.5685785536159601, Step: 285000, Train Loss: 1.3299572987452155
Epoch: 0.57356608478803, Step: 287500, Train Loss: 1.327316541373146
Epoch: 0.5785536159600998, Step: 290000, Train Loss: 1.3247047888549486
Epoch: 0.5835411471321695, Step: 292500, Train Loss: 1.3221221455276901
Epoch: 0.5885286783042394, Step: 295000, Train Loss: 1.3196057830108916
Epoch: 0.5935162094763092, Step: 297500, Train Loss: 1.317088024059009
Running evaluation (step 300000)...
Epoch: 0.5985037406483791, Step: 300000, Eval Loss: 0.942263126373291
Epoch: 0.5985037406483791, Step: 300000, Train Loss: 1.3145837550535582
Epoch: 0.6034912718204489, Step: 302500, Train Loss: 1.3121510315313258
Epoch: 0.6084788029925187, Step: 305000, Train Loss: 1.3097591757319795
Epoch: 0.6134663341645885, Step: 307500, Train Loss: 1.3073833596396214
Epoch: 0.6184538653366584, Step: 310000, Train Loss: 1.305034895829167
Epoch: 0.6234413965087282, Step: 312500, Train Loss: 1.3027173884468053
Epoch: 0.628428927680798, Step: 315000, Train Loss: 1.3004100244465577
Epoch: 0.6334164588528678, Step: 317500, Train Loss: 1.298156887787438
Epoch: 0.6384039900249376, Step: 320000, Train Loss: 1.2959175253247395
Epoch: 0.6433915211970075, Step: 322500, Train Loss: 1.2937359663836245
Epoch: 0.6483790523690773, Step: 325000, Train Loss: 1.2915651092340734
Epoch: 0.6533665835411472, Step: 327500, Train Loss: 1.2894380135375905
Epoch: 0.6583541147132169, Step: 330000, Train Loss: 1.2873081632464016
Epoch: 0.6633416458852868, Step: 332500, Train Loss: 1.2852092260854062
Epoch: 0.6683291770573566, Step: 335000, Train Loss: 1.2831095032554443
Epoch: 0.6733167082294265, Step: 337500, Train Loss: 1.2810424466159904
Epoch: 0.6783042394014963, Step: 340000, Train Loss: 1.2790020761233751
Epoch: 0.683291770573566, Step: 342500, Train Loss: 1.2769863136941997
Epoch: 0.6882793017456359, Step: 345000, Train Loss: 1.2749708889296094
Epoch: 0.6932668329177057, Step: 347500, Train Loss: 1.2730144746019152
Running evaluation (step 350000)...
Epoch: 0.6982543640897756, Step: 350000, Eval Loss: 0.9156840443611145
Epoch: 0.6982543640897756, Step: 350000, Train Loss: 1.2710374196031549
Epoch: 0.7032418952618454, Step: 352500, Train Loss: 1.2690895709308572
Epoch: 0.7082294264339152, Step: 355000, Train Loss: 1.267166874290124
Epoch: 0.713216957605985, Step: 357500, Train Loss: 1.2652844640768899
Epoch: 0.7182044887780549, Step: 360000, Train Loss: 1.2634058957757868
Epoch: 0.7231920199501247, Step: 362500, Train Loss: 1.261546260913633
Epoch: 0.7281795511221946, Step: 365000, Train Loss: 1.2597085949291604
Epoch: 0.7331670822942643, Step: 367500, Train Loss: 1.257881893067259
Epoch: 0.7381546134663342, Step: 370000, Train Loss: 1.2560658204287163
Epoch: 0.743142144638404, Step: 372500, Train Loss: 1.2542987337714417
Epoch: 0.7481296758104738, Step: 375000, Train Loss: 1.2525292346467152
Epoch: 0.7531172069825436, Step: 377500, Train Loss: 1.2507803938431714
Epoch: 0.7581047381546134, Step: 380000, Train Loss: 1.2490503767902967
Epoch: 0.7630922693266833, Step: 382500, Train Loss: 1.2473371576698904
Epoch: 0.7680798004987531, Step: 385000, Train Loss: 1.245641082591902
Epoch: 0.773067331670823, Step: 387500, Train Loss: 1.2439590162197067
Epoch: 0.7780548628428927, Step: 390000, Train Loss: 1.242287042658291
Epoch: 0.7830423940149626, Step: 392500, Train Loss: 1.2406527485291452
Epoch: 0.7880299251870324, Step: 395000, Train Loss: 1.2390131391642873
Epoch: 0.7930174563591023, Step: 397500, Train Loss: 1.237420141792147
Running evaluation (step 400000)...
Epoch: 0.7980049875311721, Step: 400000, Eval Loss: 0.8934974670410156
Epoch: 0.7980049875311721, Step: 400000, Train Loss: 1.2358248059812622
Epoch: 0.8029925187032418, Step: 402500, Train Loss: 1.2342712327940752
Epoch: 0.8079800498753117, Step: 405000, Train Loss: 1.232707037311685
Epoch: 0.8129675810473815, Step: 407500, Train Loss: 1.23117027234335
Epoch: 0.8179551122194514, Step: 410000, Train Loss: 1.2296125426302271
Epoch: 0.8229426433915212, Step: 412500, Train Loss: 1.2280992363946859
Epoch: 0.827930174563591, Step: 415000, Train Loss: 1.2266047026362703
Epoch: 0.8329177057356608, Step: 417500, Train Loss: 1.2251255175034537
Epoch: 0.8379052369077307, Step: 420000, Train Loss: 1.2236428709261826
Epoch: 0.8428927680798005, Step: 422500, Train Loss: 1.2221926358460187
Epoch: 0.8478802992518704, Step: 425000, Train Loss: 1.220748317323404
Epoch: 0.8528678304239401, Step: 427500, Train Loss: 1.2193054992224417
Epoch: 0.85785536159601, Step: 430000, Train Loss: 1.217872635301426
Epoch: 0.8628428927680798, Step: 432500, Train Loss: 1.2164649632202484
Epoch: 0.8678304239401496, Step: 435000, Train Loss: 1.2150673123395643
Epoch: 0.8728179551122195, Step: 437500, Train Loss: 1.2136697724575418
Epoch: 0.8778054862842892, Step: 440000, Train Loss: 1.2123017659449327
Epoch: 0.8827930174563591, Step: 442500, Train Loss: 1.2109512397253561
Epoch: 0.8877805486284289, Step: 445000, Train Loss: 1.209596686681752
Epoch: 0.8927680798004988, Step: 447500, Train Loss: 1.2082636846155
Running evaluation (step 450000)...
Epoch: 0.8977556109725686, Step: 450000, Eval Loss: 0.8759562373161316
Epoch: 0.8977556109725686, Step: 450000, Train Loss: 1.2069320738177698
Epoch: 0.9027431421446384, Step: 452500, Train Loss: 1.2056170024529527
Epoch: 0.9077306733167082, Step: 455000, Train Loss: 1.2043264792763955
Epoch: 0.912718204488778, Step: 457500, Train Loss: 1.2030267575128788
Epoch: 0.9177057356608479, Step: 460000, Train Loss: 1.2017449046737678
Epoch: 0.9226932668329177, Step: 462500, Train Loss: 1.20048490731193
Epoch: 0.9276807980049875, Step: 465000, Train Loss: 1.1992358842438953
Epoch: 0.9326683291770573, Step: 467500, Train Loss: 1.1979878461081892
Epoch: 0.9376558603491272, Step: 470000, Train Loss: 1.1967701574236238
Epoch: 0.942643391521197, Step: 472500, Train Loss: 1.1955733130051704
Epoch: 0.9476309226932669, Step: 475000, Train Loss: 1.1943601253091907
Epoch: 0.9526184538653366, Step: 477500, Train Loss: 1.1931736257637178
Epoch: 0.9576059850374065, Step: 480000, Train Loss: 1.1919990389180248
Epoch: 0.9625935162094763, Step: 482500, Train Loss: 1.1908377471036733
Epoch: 0.9675810473815462, Step: 485000, Train Loss: 1.1896841767915247
Epoch: 0.972568578553616, Step: 487500, Train Loss: 1.1885249717341726
Epoch: 0.9775561097256857, Step: 490000, Train Loss: 1.1873792989704106
Epoch: 0.9825436408977556, Step: 492500, Train Loss: 1.1862501514325867
Epoch: 0.9875311720698254, Step: 495000, Train Loss: 1.1851202397508755
Epoch: 0.9925187032418953, Step: 497500, Train Loss: 1.1840110409485058
Running evaluation (step 500000)...
Epoch: 0.9975062344139651, Step: 500000, Eval Loss: 0.86947101354599
Epoch: 0.9975062344139651, Step: 500000, Train Loss: 1.1829130214631236
Running evaluation (step 501250)...
Epoch: 1.0, Step: 501250, Eval Loss: 0.8694729208946228
wandb: - 0.014 MB of 0.014 MB uploadedwandb: \ 0.014 MB of 0.040 MB uploadedwandb: 
wandb: Run history:
wandb:        eval/cosine_sim_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   eval/loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval/reconstruction_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/cosine_sim_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     train/cosine_sim_weight ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:           train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         train/learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train/loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train/reconstruction_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train/reconstruction_weight ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:        eval/cosine_sim_loss 0.0133
wandb:                   eval/loss 0.86947
wandb:    eval/reconstruction_loss 12.57645
wandb:       train/cosine_sim_loss 0.01892
wandb:     train/cosine_sim_weight 21.09601
wandb:                 train/epoch 1.0
wandb:           train/global_step 501250
wandb:         train/learning_rate 0.0
wandb:                  train/loss 1.18291
wandb:   train/reconstruction_loss 13.86563
wandb: train/reconstruction_weight 0.03734
wandb: 
wandb: üöÄ View run neobert-reduced at: https://wandb.ai/cayjobla/reduced-encoders/runs/y4svqdzu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cayjobla/reduced-encoders
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250327_163649-y4svqdzu/logs
