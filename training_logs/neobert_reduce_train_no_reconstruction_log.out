You try to use a model that was created with version 3.5.0.dev0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.



/home/cayjobla/miniconda3/envs/reduced_encoders/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)
  return t.to(
wandb: Currently logged in as: cayjobla. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/cayjobla/ReducedEncoders/wandb/run-20250327_163903-xtgfblm5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neobert-reduced-no-reconstruction
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cayjobla/reduced-encoders
wandb: üöÄ View run at https://wandb.ai/cayjobla/reduced-encoders/runs/xtgfblm5
Loaded model: tomaarsen/NeoBERT-gooaq-8e-05
Initialized reduction and expansion layers with sizes: [768, 512, 256, 128, 64, 48]
Loaded dataset: sentence-transformers/gooaq, split: train, eval size: 5000
Initialized UnsupervisedCosineSimilarityLoss
Initialized optimizer and LR scheduler
Initialized wandb tracking with run name: neobert-reduced-no-reconstruction
Starting training loop...
Running evaluation (step 0)...
Epoch: 0.0, Step: 0, Eval Loss: 0.7514936327934265
Epoch: 0.0, Step: 0, Train Loss: 0.5232874751091003
Epoch: 0.004987531172069825, Step: 2500, Train Loss: 0.04462389169824643
Epoch: 0.00997506234413965, Step: 5000, Train Loss: 0.03747713534596103
Epoch: 0.014962593516209476, Step: 7500, Train Loss: 0.034211124198275586
Epoch: 0.0199501246882793, Step: 10000, Train Loss: 0.032214919216826565
Epoch: 0.02493765586034913, Step: 12500, Train Loss: 0.030869282110163678
Epoch: 0.029925187032418952, Step: 15000, Train Loss: 0.02986334187598423
Epoch: 0.034912718204488775, Step: 17500, Train Loss: 0.02906513067311348
Epoch: 0.0399002493765586, Step: 20000, Train Loss: 0.028405220106763367
Epoch: 0.04488778054862843, Step: 22500, Train Loss: 0.027838774414964094
Epoch: 0.04987531172069826, Step: 25000, Train Loss: 0.027398509161837312
Epoch: 0.05486284289276808, Step: 27500, Train Loss: 0.026999483116877047
Epoch: 0.059850374064837904, Step: 30000, Train Loss: 0.026643696713059307
Epoch: 0.06483790523690773, Step: 32500, Train Loss: 0.026331099840449294
Epoch: 0.06982543640897755, Step: 35000, Train Loss: 0.026046484399702183
Epoch: 0.07481296758104738, Step: 37500, Train Loss: 0.02579187840731031
Epoch: 0.0798004987531172, Step: 40000, Train Loss: 0.025567901512137486
Epoch: 0.08478802992518704, Step: 42500, Train Loss: 0.025343505884438865
Epoch: 0.08977556109725686, Step: 45000, Train Loss: 0.02513968123824258
Epoch: 0.09476309226932668, Step: 47500, Train Loss: 0.02495252882952324
Running evaluation (step 50000)...
Epoch: 0.09975062344139651, Step: 50000, Eval Loss: 0.01809716410934925
Epoch: 0.09975062344139651, Step: 50000, Train Loss: 0.024789426203543175
Epoch: 0.10473815461346633, Step: 52500, Train Loss: 0.024627653910148175
Epoch: 0.10972568578553615, Step: 55000, Train Loss: 0.024484587894870034
Epoch: 0.11471321695760599, Step: 57500, Train Loss: 0.02434759587907906
Epoch: 0.11970074812967581, Step: 60000, Train Loss: 0.02422114014623687
Epoch: 0.12468827930174564, Step: 62500, Train Loss: 0.02409861984991977
Epoch: 0.12967581047381546, Step: 65000, Train Loss: 0.0239807456599178
Epoch: 0.13466334164588528, Step: 67500, Train Loss: 0.023867418058161383
Epoch: 0.1396508728179551, Step: 70000, Train Loss: 0.023762118547123578
Epoch: 0.14463840399002495, Step: 72500, Train Loss: 0.02365656441684393
Epoch: 0.14962593516209477, Step: 75000, Train Loss: 0.023560910673697765
Epoch: 0.1546134663341646, Step: 77500, Train Loss: 0.0234642483062228
Epoch: 0.1596009975062344, Step: 80000, Train Loss: 0.02337661960653291
Epoch: 0.16458852867830423, Step: 82500, Train Loss: 0.023292900644151054
Epoch: 0.16957605985037408, Step: 85000, Train Loss: 0.023211066436517627
Epoch: 0.1745635910224439, Step: 87500, Train Loss: 0.023133256330938767
Epoch: 0.17955112219451372, Step: 90000, Train Loss: 0.023053800926836027
Epoch: 0.18453865336658354, Step: 92500, Train Loss: 0.02297530028651054
Epoch: 0.18952618453865336, Step: 95000, Train Loss: 0.022905158487088264
Epoch: 0.19451371571072318, Step: 97500, Train Loss: 0.02283384288653913
Running evaluation (step 100000)...
Epoch: 0.19950124688279303, Step: 100000, Eval Loss: 0.017046915367245674
Epoch: 0.19950124688279303, Step: 100000, Train Loss: 0.022760825006381998
Epoch: 0.20448877805486285, Step: 102500, Train Loss: 0.022697304866135602
Epoch: 0.20947630922693267, Step: 105000, Train Loss: 0.022631495575594945
Epoch: 0.2144638403990025, Step: 107500, Train Loss: 0.0225683765194897
Epoch: 0.2194513715710723, Step: 110000, Train Loss: 0.02250687641455255
Epoch: 0.22443890274314215, Step: 112500, Train Loss: 0.022449640935382858
Epoch: 0.22942643391521197, Step: 115000, Train Loss: 0.022393173027099707
Epoch: 0.2344139650872818, Step: 117500, Train Loss: 0.022340277466413542
Epoch: 0.23940149625935161, Step: 120000, Train Loss: 0.02228896763085885
Epoch: 0.24438902743142144, Step: 122500, Train Loss: 0.022232244304852284
Epoch: 0.24937655860349128, Step: 125000, Train Loss: 0.022178531524614298
Epoch: 0.2543640897755611, Step: 127500, Train Loss: 0.0221287807240032
Epoch: 0.2593516209476309, Step: 130000, Train Loss: 0.02208656955710625
Epoch: 0.26433915211970077, Step: 132500, Train Loss: 0.02204250131292258
Epoch: 0.26932668329177056, Step: 135000, Train Loss: 0.02199514681246023
Epoch: 0.2743142144638404, Step: 137500, Train Loss: 0.021948605165218463
Epoch: 0.2793017456359102, Step: 140000, Train Loss: 0.02190485602814128
Epoch: 0.28428927680798005, Step: 142500, Train Loss: 0.02185734975011798
Epoch: 0.2892768079800499, Step: 145000, Train Loss: 0.021815910584690734
Epoch: 0.2942643391521197, Step: 147500, Train Loss: 0.021774310956151163
Running evaluation (step 150000)...
Epoch: 0.29925187032418954, Step: 150000, Eval Loss: 0.01625215634703636
Epoch: 0.29925187032418954, Step: 150000, Train Loss: 0.021733447067265113
Epoch: 0.30423940149625933, Step: 152500, Train Loss: 0.021692475008792424
Epoch: 0.3092269326683292, Step: 155000, Train Loss: 0.021653613357775856
Epoch: 0.314214463840399, Step: 157500, Train Loss: 0.021614883837578095
Epoch: 0.3192019950124688, Step: 160000, Train Loss: 0.02157877213092784
Epoch: 0.32418952618453867, Step: 162500, Train Loss: 0.021542681021775683
Epoch: 0.32917705735660846, Step: 165000, Train Loss: 0.021503150758407915
Epoch: 0.3341645885286783, Step: 167500, Train Loss: 0.021466259083854158
Epoch: 0.33915211970074816, Step: 170000, Train Loss: 0.021430058456190067
Epoch: 0.34413965087281795, Step: 172500, Train Loss: 0.02139246855716554
Epoch: 0.3491271820448878, Step: 175000, Train Loss: 0.02135770358611907
Epoch: 0.3541147132169576, Step: 177500, Train Loss: 0.021325029038954908
Epoch: 0.35910224438902744, Step: 180000, Train Loss: 0.02129224605824633
Epoch: 0.3640897755610973, Step: 182500, Train Loss: 0.021259519982119727
Epoch: 0.3690773067331671, Step: 185000, Train Loss: 0.021227234637711406
Epoch: 0.3740648379052369, Step: 187500, Train Loss: 0.02119522994668353
Epoch: 0.3790523690773067, Step: 190000, Train Loss: 0.021162287295614805
Epoch: 0.38403990024937656, Step: 192500, Train Loss: 0.02112811943619185
Epoch: 0.38902743142144636, Step: 195000, Train Loss: 0.021100199505361017
Epoch: 0.3940149625935162, Step: 197500, Train Loss: 0.021068813907356605
Running evaluation (step 200000)...
Epoch: 0.39900249376558605, Step: 200000, Eval Loss: 0.015526927076280117
Epoch: 0.39900249376558605, Step: 200000, Train Loss: 0.021040473054491842
Epoch: 0.40399002493765584, Step: 202500, Train Loss: 0.021011364483167247
Epoch: 0.4089775561097257, Step: 205000, Train Loss: 0.020980550424697916
Epoch: 0.4139650872817955, Step: 207500, Train Loss: 0.02095063463552194
Epoch: 0.41895261845386533, Step: 210000, Train Loss: 0.020923481696525227
Epoch: 0.4239401496259352, Step: 212500, Train Loss: 0.020894039977069744
Epoch: 0.428927680798005, Step: 215000, Train Loss: 0.02086629454839555
Epoch: 0.4339152119700748, Step: 217500, Train Loss: 0.020838964194648462
Epoch: 0.4389027431421446, Step: 220000, Train Loss: 0.020811967442104753
Epoch: 0.44389027431421446, Step: 222500, Train Loss: 0.02078194371698716
Epoch: 0.4488778054862843, Step: 225000, Train Loss: 0.02075561924803187
Epoch: 0.4538653366583541, Step: 227500, Train Loss: 0.020728637445746788
Epoch: 0.45885286783042395, Step: 230000, Train Loss: 0.02070482003986133
Epoch: 0.46384039900249374, Step: 232500, Train Loss: 0.020677833131901985
Epoch: 0.4688279301745636, Step: 235000, Train Loss: 0.020653761352846807
Epoch: 0.47381546134663344, Step: 237500, Train Loss: 0.020628042662123498
Epoch: 0.47880299251870323, Step: 240000, Train Loss: 0.02060388169184052
Epoch: 0.4837905236907731, Step: 242500, Train Loss: 0.020576976062951512
Epoch: 0.48877805486284287, Step: 245000, Train Loss: 0.020552713267349955
Epoch: 0.4937655860349127, Step: 247500, Train Loss: 0.02052933489384715
Running evaluation (step 250000)...
Epoch: 0.49875311720698257, Step: 250000, Eval Loss: 0.0153999924659729
Epoch: 0.49875311720698257, Step: 250000, Train Loss: 0.0205073415815668
Epoch: 0.5037406483790524, Step: 252500, Train Loss: 0.020484201641887818
Epoch: 0.5087281795511222, Step: 255000, Train Loss: 0.020461280955205746
Epoch: 0.513715710723192, Step: 257500, Train Loss: 0.020438406881913236
Epoch: 0.5187032418952618, Step: 260000, Train Loss: 0.020414103526995457
Epoch: 0.5236907730673317, Step: 262500, Train Loss: 0.02039034514820153
Epoch: 0.5286783042394015, Step: 265000, Train Loss: 0.02036840675187941
Epoch: 0.5336658354114713, Step: 267500, Train Loss: 0.020344348242297177
Epoch: 0.5386533665835411, Step: 270000, Train Loss: 0.02032292171407791
Epoch: 0.543640897755611, Step: 272500, Train Loss: 0.020300469534961434
Epoch: 0.5486284289276808, Step: 275000, Train Loss: 0.02027936798074345
Epoch: 0.5536159600997507, Step: 277500, Train Loss: 0.02025736750959934
Epoch: 0.5586034912718204, Step: 280000, Train Loss: 0.02023588918840623
Epoch: 0.5635910224438903, Step: 282500, Train Loss: 0.02021438571729494
Epoch: 0.5685785536159601, Step: 285000, Train Loss: 0.02019292125614784
Epoch: 0.57356608478803, Step: 287500, Train Loss: 0.020171087832202596
Epoch: 0.5785536159600998, Step: 290000, Train Loss: 0.020149409516431176
Epoch: 0.5835411471321695, Step: 292500, Train Loss: 0.020129532033568195
Epoch: 0.5885286783042394, Step: 295000, Train Loss: 0.020108919990091352
Epoch: 0.5935162094763092, Step: 297500, Train Loss: 0.02008783774240823
Running evaluation (step 300000)...
Epoch: 0.5985037406483791, Step: 300000, Eval Loss: 0.014519347809255123
Epoch: 0.5985037406483791, Step: 300000, Train Loss: 0.020066756834236952
Epoch: 0.6034912718204489, Step: 302500, Train Loss: 0.020045851773616428
Epoch: 0.6084788029925187, Step: 305000, Train Loss: 0.020024717760107516
Epoch: 0.6134663341645885, Step: 307500, Train Loss: 0.02000451853757248
Epoch: 0.6184538653366584, Step: 310000, Train Loss: 0.019984057451771696
Epoch: 0.6234413965087282, Step: 312500, Train Loss: 0.019964856018604586
Epoch: 0.628428927680798, Step: 315000, Train Loss: 0.01994497539054911
Epoch: 0.6334164588528678, Step: 317500, Train Loss: 0.019924998099832662
Epoch: 0.6384039900249376, Step: 320000, Train Loss: 0.019903420203159584
Epoch: 0.6433915211970075, Step: 322500, Train Loss: 0.01988419606434412
Epoch: 0.6483790523690773, Step: 325000, Train Loss: 0.0198647200149951
Epoch: 0.6533665835411472, Step: 327500, Train Loss: 0.01984533629595553
Epoch: 0.6583541147132169, Step: 330000, Train Loss: 0.019826657986696215
Epoch: 0.6633416458852868, Step: 332500, Train Loss: 0.01980825915511301
Epoch: 0.6683291770573566, Step: 335000, Train Loss: 0.01978897771984052
Epoch: 0.6733167082294265, Step: 337500, Train Loss: 0.019770782283336667
Epoch: 0.6783042394014963, Step: 340000, Train Loss: 0.019751394720052058
Epoch: 0.683291770573566, Step: 342500, Train Loss: 0.019733264705734156
Epoch: 0.6882793017456359, Step: 345000, Train Loss: 0.01971403758216036
Epoch: 0.6932668329177057, Step: 347500, Train Loss: 0.01969763598749858
Running evaluation (step 350000)...
Epoch: 0.6982543640897756, Step: 350000, Eval Loss: 0.014198582619428635
Epoch: 0.6982543640897756, Step: 350000, Train Loss: 0.019680287000333994
Epoch: 0.7032418952618454, Step: 352500, Train Loss: 0.019662493235897003
Epoch: 0.7082294264339152, Step: 355000, Train Loss: 0.019644334284463227
Epoch: 0.713216957605985, Step: 357500, Train Loss: 0.019625461542107807
Epoch: 0.7182044887780549, Step: 360000, Train Loss: 0.01960769846400191
Epoch: 0.7231920199501247, Step: 362500, Train Loss: 0.019590686899120736
Epoch: 0.7281795511221946, Step: 365000, Train Loss: 0.01957323301380317
Epoch: 0.7331670822942643, Step: 367500, Train Loss: 0.01955637872930244
Epoch: 0.7381546134663342, Step: 370000, Train Loss: 0.019538724782441248
Epoch: 0.743142144638404, Step: 372500, Train Loss: 0.019520955711984214
Epoch: 0.7481296758104738, Step: 375000, Train Loss: 0.019503496278245828
Epoch: 0.7531172069825436, Step: 377500, Train Loss: 0.01948648290391346
Epoch: 0.7581047381546134, Step: 380000, Train Loss: 0.019469765649423
Epoch: 0.7630922693266833, Step: 382500, Train Loss: 0.019453192310778595
Epoch: 0.7680798004987531, Step: 385000, Train Loss: 0.019437080058919205
Epoch: 0.773067331670823, Step: 387500, Train Loss: 0.019420501350139074
Epoch: 0.7780548628428927, Step: 390000, Train Loss: 0.019403915922147734
Epoch: 0.7830423940149626, Step: 392500, Train Loss: 0.019386896951800094
Epoch: 0.7880299251870324, Step: 395000, Train Loss: 0.01937062592214702
Epoch: 0.7930174563591023, Step: 397500, Train Loss: 0.01935451870787392
Running evaluation (step 400000)...
Epoch: 0.7980049875311721, Step: 400000, Eval Loss: 0.013706088997423649
Epoch: 0.7980049875311721, Step: 400000, Train Loss: 0.01933851028819962
Epoch: 0.8029925187032418, Step: 402500, Train Loss: 0.019321884643594994
Epoch: 0.8079800498753117, Step: 405000, Train Loss: 0.01930453134078695
Epoch: 0.8129675810473815, Step: 407500, Train Loss: 0.019289624535724033
Epoch: 0.8179551122194514, Step: 410000, Train Loss: 0.019273842797621173
Epoch: 0.8229426433915212, Step: 412500, Train Loss: 0.01925864414116382
Epoch: 0.827930174563591, Step: 415000, Train Loss: 0.019243602057988758
Epoch: 0.8329177057356608, Step: 417500, Train Loss: 0.01922791239486031
Epoch: 0.8379052369077307, Step: 420000, Train Loss: 0.019211707998684942
Epoch: 0.8428927680798005, Step: 422500, Train Loss: 0.01919683097064561
Epoch: 0.8478802992518704, Step: 425000, Train Loss: 0.019182152662215672
Epoch: 0.8528678304239401, Step: 427500, Train Loss: 0.019166378194844465
Epoch: 0.85785536159601, Step: 430000, Train Loss: 0.0191514196645993
Epoch: 0.8628428927680798, Step: 432500, Train Loss: 0.019136135910174757
Epoch: 0.8678304239401496, Step: 435000, Train Loss: 0.019120362299156833
Epoch: 0.8728179551122195, Step: 437500, Train Loss: 0.019105234477225553
Epoch: 0.8778054862842892, Step: 440000, Train Loss: 0.019090875114452757
Epoch: 0.8827930174563591, Step: 442500, Train Loss: 0.019075654457599997
Epoch: 0.8877805486284289, Step: 445000, Train Loss: 0.0190601490183081
Epoch: 0.8927680798004988, Step: 447500, Train Loss: 0.019045755320727997
Running evaluation (step 450000)...
Epoch: 0.8977556109725686, Step: 450000, Eval Loss: 0.013396981172263622
Epoch: 0.8977556109725686, Step: 450000, Train Loss: 0.01903169189820051
Epoch: 0.9027431421446384, Step: 452500, Train Loss: 0.019018224524817558
Epoch: 0.9077306733167082, Step: 455000, Train Loss: 0.01900246523583864
Epoch: 0.912718204488778, Step: 457500, Train Loss: 0.018987954439626215
Epoch: 0.9177057356608479, Step: 460000, Train Loss: 0.01897389479560351
Epoch: 0.9226932668329177, Step: 462500, Train Loss: 0.018959802260213283
Epoch: 0.9276807980049875, Step: 465000, Train Loss: 0.018945618188437013
Epoch: 0.9326683291770573, Step: 467500, Train Loss: 0.0189320219633826
Epoch: 0.9376558603491272, Step: 470000, Train Loss: 0.018918418919724792
Epoch: 0.942643391521197, Step: 472500, Train Loss: 0.01890501552332301
Epoch: 0.9476309226932669, Step: 475000, Train Loss: 0.018891248437478834
Epoch: 0.9526184538653366, Step: 477500, Train Loss: 0.018877379887258874
Epoch: 0.9576059850374065, Step: 480000, Train Loss: 0.0188638115304951
Epoch: 0.9625935162094763, Step: 482500, Train Loss: 0.018850988960656456
Epoch: 0.9675810473815462, Step: 485000, Train Loss: 0.018837742461847024
Epoch: 0.972568578553616, Step: 487500, Train Loss: 0.018824729763476374
Epoch: 0.9775561097256857, Step: 490000, Train Loss: 0.018811513211494487
Epoch: 0.9825436408977556, Step: 492500, Train Loss: 0.01879905220614679
Epoch: 0.9875311720698254, Step: 495000, Train Loss: 0.01878572586900214
Epoch: 0.9925187032418953, Step: 497500, Train Loss: 0.018772563783628113
Running evaluation (step 500000)...
Epoch: 0.9975062344139651, Step: 500000, Eval Loss: 0.013050603680312634
Epoch: 0.9975062344139651, Step: 500000, Train Loss: 0.018760344988813536
Running evaluation (step 501250)...
Epoch: 1.0, Step: 501250, Eval Loss: 0.013048963621258736
wandb: - 0.014 MB of 0.041 MB uploadedwandb: \ 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:  eval/cosine_sim_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             eval/loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train/cosine_sim_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:     train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   train/learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:  eval/cosine_sim_loss 0.01305
wandb:             eval/loss 0.01305
wandb: train/cosine_sim_loss 0.01876
wandb:           train/epoch 1.0
wandb:     train/global_step 501250
wandb:   train/learning_rate 0.0
wandb:            train/loss 0.01876
wandb: 
wandb: üöÄ View run neobert-reduced-no-reconstruction at: https://wandb.ai/cayjobla/reduced-encoders/runs/xtgfblm5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cayjobla/reduced-encoders
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250327_163903-xtgfblm5/logs
